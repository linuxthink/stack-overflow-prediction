{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading time: 21.3262369633\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "# load all_data and test_data\n",
    "start_time = time.time()\n",
    "all_data = pickle.load(open('all_data.pickle', 'rb'))\n",
    "print('data loading time:', time.time() - start_time)\n",
    "\n",
    "# remove the outlier\n",
    "for i in reversed(range(len(all_data))):\n",
    "    d = all_data[i]\n",
    "    if d['helpful']['outOf'] > 3000:\n",
    "        all_data.pop(i)\n",
    "    elif d['helpful']['outOf'] < d['helpful']['nHelpful']:\n",
    "        all_data.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.mean(np.fabs(helpfuls_predict - helpfuls.astype(float)))\n",
    "\n",
    "# load pre computed features\n",
    "global_feature, users_feature, items_feature = pickle.load(\n",
    "    open('global_users_items_feature.feature', 'rb'))\n",
    "style_dict = pickle.load(open('style_dict.feature', 'rb'))\n",
    "\n",
    "# feature engineering\n",
    "def get_feature_time(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(\n",
    "        unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = float(y)\n",
    "    m = float(m)\n",
    "    d = float(d)\n",
    "    return [y, m, d]\n",
    "\n",
    "def get_feature_style(d):\n",
    "    # load from style dict\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    s = style_dict[user_id][item_id]\n",
    "\n",
    "    feature = [s['num_words'],\n",
    "               s['num_words_summary'],\n",
    "               s['redability'],\n",
    "               s['avg_word_len'],\n",
    "               s['num_words'] /\n",
    "               s['num_sentences'] if s['num_sentences'] != 0.0 else 0.0,\n",
    "               s['num_unique_words'],\n",
    "               s['exclam_exclam_count'] + s['question_count'],\n",
    "               s['dotdotdot_count'],\n",
    "               s['capital_ratio']\n",
    "               ]\n",
    "    return feature\n",
    "\n",
    "def get_time_spot_ratio(times, spot):\n",
    "    # return the array index ratio to insert spot\n",
    "    if len(times) == 0:\n",
    "        return 0.\n",
    "    index = np.searchsorted(np.array(times), spot)\n",
    "    return float(index) / float(len(times))\n",
    "\n",
    "def get_feature_user(d):\n",
    "    user_id = d['reviewerID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "\n",
    "    s = users_feature[user_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "               ]\n",
    "    return feature\n",
    "\n",
    "def get_feature_item(d):\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "\n",
    "    s = items_feature[item_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "               ]\n",
    "    return feature\n",
    "\n",
    "def get_feature(d):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "\n",
    "    # offset\n",
    "    feature = [1.0]\n",
    "\n",
    "    # user\n",
    "    feature += get_feature_user(d)\n",
    "    # item\n",
    "    feature += get_feature_item(d)\n",
    "\n",
    "    # outof\n",
    "    feature += [float(d['helpful']['outOf'])]\n",
    "    # rating\n",
    "    feature += [float(d['rating'])]\n",
    "    # styles\n",
    "    feature += get_feature_style(d)\n",
    "    # time\n",
    "    feature += get_feature_time(d)\n",
    "\n",
    "    return feature\n",
    "\n",
    "# get [feature, label] from single datum\n",
    "def get_feature_label_weight(d, total_outof_weights):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    assert outof != 0.\n",
    "\n",
    "    # feature\n",
    "    feature = get_feature(d)\n",
    "    # label\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / \\\n",
    "        float(d['helpful']['outOf'])\n",
    "    # weight\n",
    "    weight = float(d['helpful']['outOf']) / total_outof_weights\n",
    "\n",
    "    return (feature, ratio_label, weight)\n",
    "\n",
    "# build [feature, label] list from entire dataset\n",
    "def make_dataset(train_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    weights = []\n",
    "\n",
    "    train_outofs = np.array([d['helpful']['outOf']\n",
    "                             for d in train_data]).astype(float)\n",
    "    total_outof_weights = np.sum(train_outofs)\n",
    "\n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label, weight = get_feature_label_weight(\n",
    "            d, total_outof_weights)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "\n",
    "    return (np.array(features), np.array(labels), np.array(weights))\n",
    "\n",
    "# make one prediction\n",
    "def predict_helpful(d, ratio_predictor):\n",
    "    # ratio_predictor[func]: y = ratio_predictor(get_feature(d))\n",
    "\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "\n",
    "    if (user_id in users_feature) and (item_id in items_feature):\n",
    "        predict = ratio_predictor(np.array(get_feature(d)).reshape((1, -1)))\n",
    "        ratio = predict[0]  # np.ndarray\n",
    "    elif (user_id in users_feature) and (item_id not in items_feature):\n",
    "        ratio = users_feature[user_id]['ratio_b']\n",
    "    elif (user_id not in users_feature) and (item_id in items_feature):\n",
    "        ratio = items_ratio[item_id]['ratio_b']\n",
    "    else:\n",
    "        ratio = global_feature['global_ratio_b']\n",
    "    return ratio * outof\n",
    "\n",
    "# make predictions and get mae on a dataset\n",
    "def get_valid_mae(valid_data, ratio_predictor):\n",
    "    print(len(valid_data))\n",
    "    # ground truth nhelpful\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    # predited nhelpful\n",
    "    helpfuls_predict = np.array(\n",
    "        [predict_helpful(d, ratio_predictor) for d in valid_data])\n",
    "    print(len(helpfuls), len(helpfuls_predict))\n",
    "    # return mae\n",
    "    return get_mae(helpfuls, helpfuls_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset prepared\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "# all_xs, all_ys, all_weights = make_dataset(all_data)\n",
    "pickle.dump((all_xs, all_ys, all_weights), \n",
    "            open(\"all_xs_all_ys_all_weights.pickle\", \"wb\"),\n",
    "            protocol=pickle.HIGHEST_PROTOCOL)\n",
    "all_xs, all_ys, all_weights = pickle.load(open(\"all_xs_all_ys_all_weights.pickle\", \"rb\"))\n",
    "print('dataset prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting regressor\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2595            3.59m\n",
      "         2           0.2593            3.48m\n",
      "         3           0.2592            3.42m\n",
      "         4           0.2591            3.40m\n",
      "         5           0.2589            3.38m\n",
      "         6           0.2588            3.36m\n",
      "         7           0.2587            3.35m\n",
      "         8           0.2585            3.34m\n",
      "         9           0.2584            3.33m\n",
      "        10           0.2583            3.33m\n",
      "        20           0.2569            3.28m\n",
      "        30           0.2556            3.24m\n",
      "        40           0.2543            3.20m\n",
      "        50           0.2530            3.17m\n",
      "        60           0.2517            3.13m\n",
      "        70           0.2505            3.09m\n",
      "        80           0.2492            3.06m\n",
      "        90           0.2480            3.02m\n",
      "       100           0.2467            2.99m\n",
      "       200           0.2352            2.66m\n",
      "       300           0.2249            2.34m\n",
      "       400           0.2158            2.01m\n",
      "       500           0.2077            1.67m\n",
      "       600           0.2005            1.34m\n",
      "       700           0.1940            1.01m\n",
      "       800           0.1882           40.24s\n",
      "       900           0.1831           20.14s\n",
      "      1000           0.1787            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
       "             loss='lad', max_depth=6, max_features=None,\n",
       "             max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=1,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call gradient boosting\n",
    "print('start fitting regressor')\n",
    "regressor = GradientBoostingRegressor(learning_rate=0.001,\n",
    "                                      n_estimators=1000,\n",
    "                                      max_depth=6,\n",
    "                                      loss='lad',\n",
    "                                      verbose=1)\n",
    "regressor.fit(all_xs[:30000], all_ys[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25945485,  0.25932139,  0.25918807,  0.25905488,  0.25892183,\n",
       "        0.25878893,  0.25865602,  0.25852199,  0.25838809,  0.25825433,\n",
       "        0.25811963,  0.25798506,  0.25785063,  0.25771634,  0.25758221,\n",
       "        0.25744733,  0.25731258,  0.25717797,  0.25704349,  0.25690925,\n",
       "        0.25677354,  0.25663798,  0.25650257,  0.25636729,  0.25623214,\n",
       "        0.25610106,  0.25597016,  0.2558394 ,  0.25570879,  0.25557843,\n",
       "        0.25544827,  0.25531832,  0.25518477,  0.25505523,  0.25492584,\n",
       "        0.2547966 ,  0.25466749,  0.25453851,  0.25440967,  0.2542809 ,\n",
       "        0.25415227,  0.2540238 ,  0.25389548,  0.2537673 ,  0.25363931,\n",
       "        0.25350749,  0.25337655,  0.25324588,  0.25311533,  0.25298494,\n",
       "        0.2528547 ,  0.25272459,  0.25259472,  0.25246617,  0.25233774,\n",
       "        0.25220951,  0.25208143,  0.25195349,  0.25182576,  0.25170044,\n",
       "        0.25157526,  0.25145021,  0.2513253 ,  0.25120052,  0.25107586,\n",
       "        0.25095132,  0.25082695,  0.25070277,  0.25057871,  0.25045478,\n",
       "        0.25033099,  0.25020732,  0.25008379,  0.24996045,  0.24983725,\n",
       "        0.24971418,  0.24959124,  0.24946843,  0.24934578,  0.24922332,\n",
       "        0.24909817,  0.24897319,  0.24884836,  0.24872259,  0.24859695,\n",
       "        0.24847146,  0.24834754,  0.24822307,  0.24809944,  0.24797595,\n",
       "        0.2478523 ,  0.24772677,  0.24760136,  0.24747817,  0.24735547,\n",
       "        0.2472326 ,  0.24710876,  0.24698833,  0.24686746,  0.24674726,\n",
       "        0.2466272 ,  0.24650896,  0.24639055,  0.24627228,  0.24615275,\n",
       "        0.24603338,  0.24591282,  0.24579254,  0.24567238,  0.24555357,\n",
       "        0.24543365,  0.24531511,  0.24519436,  0.24507543,  0.24495732,\n",
       "        0.24483714,  0.24471714,  0.2445995 ,  0.24447925,  0.24436022,\n",
       "        0.24424023,  0.24412035,  0.2440017 ,  0.24388254,  0.24376352,\n",
       "        0.24364465,  0.24352595,  0.24340739,  0.24328949,  0.24317172,\n",
       "        0.24305346,  0.24293578,  0.24281784,  0.24270007,  0.2425817 ,\n",
       "        0.24246352,  0.24234706,  0.24223074,  0.24211454,  0.24199847,\n",
       "        0.24188251,  0.24176668,  0.24165101,  0.24153516,  0.24141943,\n",
       "        0.24130387,  0.24118844,  0.24107313,  0.24095797,  0.24084222,\n",
       "        0.24072661,  0.24061043,  0.24049488,  0.24038096,  0.24026717,\n",
       "        0.24015351,  0.24004001,  0.23992667,  0.23981374,  0.23970092,\n",
       "        0.23958822,  0.23947565,  0.23936031,  0.23924513,  0.23913158,\n",
       "        0.23901814,  0.23890483,  0.23879164,  0.23867857,  0.23856558,\n",
       "        0.23845135,  0.23833725,  0.23822413,  0.23811116,  0.23799831,\n",
       "        0.2378856 ,  0.23777285,  0.2376602 ,  0.2375477 ,  0.23743533,\n",
       "        0.23732312,  0.23721214,  0.23709976,  0.23698754,  0.23687548,\n",
       "        0.23676356,  0.23665183,  0.23654086,  0.23643002,  0.23631906,\n",
       "        0.23620824,  0.23609717,  0.2359868 ,  0.23587601,  0.23576537,\n",
       "        0.23565487,  0.23554596,  0.23543721,  0.23532762,  0.23521909,\n",
       "        0.23511067,  0.23500143,  0.23489327,  0.23478409,  0.23467518,\n",
       "        0.23456623,  0.23445757,  0.23434902,  0.2342404 ,  0.23413206,\n",
       "        0.23402366,  0.23391554,  0.23380735,  0.23369946,  0.23359171,\n",
       "        0.23348391,  0.23337639,  0.23326909,  0.23316187,  0.2330546 ,\n",
       "        0.23294772,  0.23284072,  0.2327341 ,  0.23262762,  0.23252136,\n",
       "        0.23241521,  0.23230924,  0.23220343,  0.23209745,  0.23199159,\n",
       "        0.23188601,  0.23177928,  0.23167398,  0.23156756,  0.23146218,\n",
       "        0.23135725,  0.23125225,  0.23114776,  0.23104258,  0.23093836,\n",
       "        0.23083509,  0.23073193,  0.23062713,  0.23052298,  0.23042021,\n",
       "        0.2303175 ,  0.23021321,  0.23011072,  0.2300085 ,  0.2299045 ,\n",
       "        0.22980249,  0.22969872,  0.22959692,  0.22949508,  0.22939233,\n",
       "        0.22929077,  0.2291883 ,  0.2290873 ,  0.22898641,  0.22888566,\n",
       "        0.22878487,  0.22868435,  0.22858395,  0.22848365,  0.22838435,\n",
       "        0.22828426,  0.22818515,  0.22808525,  0.22798547,  0.2278858 ,\n",
       "        0.22778624,  0.22768679,  0.22758743,  0.22748671,  0.2273876 ,\n",
       "        0.22728525,  0.22718657,  0.22708443,  0.22698544,  0.22688544,\n",
       "        0.22678476,  0.22668448,  0.22658501,  0.22648568,  0.22638646,\n",
       "        0.22628884,  0.22618897,  0.2260892 ,  0.22598955,  0.22588969,\n",
       "        0.22579032,  0.22569108,  0.22559231,  0.22549365,  0.22539512,\n",
       "        0.22529653,  0.22519805,  0.22509968,  0.22500143,  0.22490291,\n",
       "        0.22480451,  0.22470622,  0.22460852,  0.22451094,  0.22441351,\n",
       "        0.22431618,  0.22421892,  0.22412177,  0.22402474,  0.22392781,\n",
       "        0.2238305 ,  0.2237333 ,  0.22363621,  0.22353926,  0.22344485,\n",
       "        0.22335054,  0.22325638,  0.22316015,  0.22306403,  0.22296887,\n",
       "        0.22287524,  0.22278029,  0.22268687,  0.22259157,  0.22249646,\n",
       "        0.22240131,  0.22230418,  0.22220926,  0.22211236,  0.22201781,\n",
       "        0.22192344,  0.22182976,  0.22173653,  0.22164433,  0.22154949,\n",
       "        0.22145754,  0.22136303,  0.22126857,  0.22117435,  0.22108265,\n",
       "        0.22099173,  0.22090092,  0.22081024,  0.22071765,  0.22062554,\n",
       "        0.22053353,  0.22044101,  0.22034867,  0.22025688,  0.2201644 ,\n",
       "        0.22007203,  0.21998117,  0.21989049,  0.21979989,  0.21971071,\n",
       "        0.21962197,  0.21953254,  0.21944321,  0.21935375,  0.21926477,\n",
       "        0.21917524,  0.21908581,  0.21899653,  0.21890783,  0.2188195 ,\n",
       "        0.21872822,  0.2186401 ,  0.21855196,  0.21846351,  0.21837524,\n",
       "        0.21828748,  0.2181999 ,  0.21811032,  0.21802304,  0.2179362 ,\n",
       "        0.21784925,  0.21776264,  0.21767548,  0.21758859,  0.2175016 ,\n",
       "        0.21741495,  0.21732868,  0.21724259,  0.21715401,  0.21706555,\n",
       "        0.21697737,  0.21688894,  0.21680039,  0.21671214,  0.21662395,\n",
       "        0.21653595,  0.21644797,  0.21636028,  0.21627294,  0.21618575,\n",
       "        0.21609914,  0.21601205,  0.21592516,  0.21583871,  0.21575207,\n",
       "        0.21566538,  0.21557877,  0.21549213,  0.21540578,  0.21531976,\n",
       "        0.21523543,  0.2151494 ,  0.21506338,  0.21497772,  0.21489447,\n",
       "        0.21481105,  0.2147274 ,  0.21464407,  0.2145607 ,  0.21447801,\n",
       "        0.21439482,  0.21431192,  0.21422894,  0.21414613,  0.21406398,\n",
       "        0.21398134,  0.21389817,  0.21381498,  0.21373204,  0.21364943,\n",
       "        0.21356689,  0.2134841 ,  0.21340156,  0.21331875,  0.21323641,\n",
       "        0.21315201,  0.21307001,  0.21299016,  0.21290786,  0.21282566,\n",
       "        0.21274354,  0.21266153,  0.21257961,  0.2124978 ,  0.21241693,\n",
       "        0.21233615,  0.21225465,  0.21217406,  0.2120936 ,  0.21201279,\n",
       "        0.21193208,  0.21185147,  0.21177094,  0.21169041,  0.21160865,\n",
       "        0.21152815,  0.2114479 ,  0.21136774,  0.21128768,  0.21120773,\n",
       "        0.21112642,  0.21104521,  0.21096544,  0.21088537,  0.21080541,\n",
       "        0.21072417,  0.21064507,  0.21056608,  0.21048718,  0.21040837,\n",
       "        0.21032966,  0.21025106,  0.21017256,  0.21009345,  0.21001574,\n",
       "        0.20993814,  0.20986063,  0.20978321,  0.20970587,  0.20962862,\n",
       "        0.20955146,  0.2094744 ,  0.20939743,  0.20932054,  0.20924375,\n",
       "        0.20916709,  0.20909053,  0.20901405,  0.20893768,  0.20886141,\n",
       "        0.2087848 ,  0.20870828,  0.2086323 ,  0.20855644,  0.20848075,\n",
       "        0.20840568,  0.20833071,  0.20825583,  0.20818097,  0.20810589,\n",
       "        0.2080309 ,  0.207956  ,  0.20788136,  0.20780628,  0.20772901,\n",
       "        0.20765126,  0.20757429,  0.20749756,  0.20742301,  0.20734867,\n",
       "        0.20727295,  0.20719653,  0.20712018,  0.207044  ,  0.20697002,\n",
       "        0.20689613,  0.20682235,  0.20674903,  0.20667582,  0.20660094,\n",
       "        0.20652563,  0.20645039,  0.2063753 ,  0.20630025,  0.20622558,\n",
       "        0.20615099,  0.2060757 ,  0.20600129,  0.20592724,  0.20585332,\n",
       "        0.20577935,  0.20570545,  0.20563165,  0.20555749,  0.20548333,\n",
       "        0.20540958,  0.20533588,  0.20526256,  0.20518894,  0.20511623,\n",
       "        0.20504353,  0.20496912,  0.20489488,  0.20482245,  0.20475017,\n",
       "        0.20467757,  0.20460455,  0.20453162,  0.20445879,  0.20438616,\n",
       "        0.20431372,  0.20424145,  0.20416842,  0.20409548,  0.20402188,\n",
       "        0.20394926,  0.20387596,  0.20380279,  0.20372946,  0.20365624,\n",
       "        0.20358313,  0.20351012,  0.20343746,  0.20336494,  0.20329252,\n",
       "        0.20322031,  0.20314896,  0.20307697,  0.20300539,  0.20293287,\n",
       "        0.20286063,  0.20278851,  0.2027168 ,  0.20264508,  0.20257348,\n",
       "        0.2025029 ,  0.20243214,  0.20236159,  0.20229099,  0.2022205 ,\n",
       "        0.20215011,  0.20207977,  0.20200951,  0.20193938,  0.20186938,\n",
       "        0.2017996 ,  0.20172977,  0.20166003,  0.20159079,  0.20152163,\n",
       "        0.20145269,  0.20138382,  0.20131462,  0.20124553,  0.20117637,\n",
       "        0.20110733,  0.20103839,  0.20096956,  0.20090106,  0.20083266,\n",
       "        0.20076442,  0.20069645,  0.20062838,  0.20056038,  0.20049246,\n",
       "        0.20042463,  0.20035689,  0.20028889,  0.20022098,  0.20015316,\n",
       "        0.20008541,  0.20001776,  0.19995019,  0.19988274,  0.19981535,\n",
       "        0.19974803,  0.19968081,  0.19961128,  0.19954184,  0.19947251,\n",
       "        0.19940727,  0.19934212,  0.19927725,  0.19921246,  0.19914775,\n",
       "        0.19907999,  0.19901231,  0.19894784,  0.19888347,  0.19881916,\n",
       "        0.19875492,  0.19868664,  0.19861845,  0.19855034,  0.1984823 ,\n",
       "        0.19841435,  0.19834649,  0.1982789 ,  0.19821138,  0.19814604,\n",
       "        0.19807872,  0.1980115 ,  0.19794791,  0.19788286,  0.19781788,\n",
       "        0.19775316,  0.19768839,  0.19762397,  0.19755725,  0.19749055,\n",
       "        0.197424  ,  0.19735754,  0.19729117,  0.19722488,  0.1971611 ,\n",
       "        0.19709503,  0.19702915,  0.19696342,  0.19689755,  0.19683202,\n",
       "        0.19676888,  0.19670362,  0.19663828,  0.19657285,  0.19651026,\n",
       "        0.19644757,  0.19638512,  0.19632258,  0.19626011,  0.19619763,\n",
       "        0.19613522,  0.19607263,  0.19600995,  0.1959473 ,  0.19588475,\n",
       "        0.19582286,  0.19576106,  0.19569714,  0.19563347,  0.19557172,\n",
       "        0.19551032,  0.19544897,  0.19538766,  0.19532657,  0.19526461,\n",
       "        0.19520291,  0.19514144,  0.19508053,  0.19501921,  0.19495782,\n",
       "        0.19489654,  0.19483546,  0.19477448,  0.19471359,  0.1946526 ,\n",
       "        0.19458966,  0.19452658,  0.19446361,  0.1944009 ,  0.19433806,\n",
       "        0.19427528,  0.1942126 ,  0.19415018,  0.19408763,  0.19402516,\n",
       "        0.19396297,  0.19390069,  0.1938385 ,  0.19377638,  0.19371427,\n",
       "        0.19365204,  0.19358989,  0.19352803,  0.19346608,  0.19340365,\n",
       "        0.1933411 ,  0.19327898,  0.19321709,  0.1931551 ,  0.19309384,\n",
       "        0.19303263,  0.1929715 ,  0.19291012,  0.19284871,  0.19278749,\n",
       "        0.19272642,  0.19266522,  0.19260412,  0.1925431 ,  0.19248215,\n",
       "        0.19242147,  0.19236089,  0.19230029,  0.1922398 ,  0.1921796 ,\n",
       "        0.19211931,  0.19205911,  0.191999  ,  0.19193896,  0.191879  ,\n",
       "        0.1918187 ,  0.19175895,  0.19169927,  0.19163967,  0.19158015,\n",
       "        0.1915207 ,  0.19146132,  0.19140204,  0.19134283,  0.19128371,\n",
       "        0.19122564,  0.19116763,  0.19110971,  0.19105185,  0.19099405,\n",
       "        0.19093642,  0.19087875,  0.19082115,  0.19076544,  0.19070803,\n",
       "        0.19065071,  0.19059346,  0.19053627,  0.1904806 ,  0.19042317,\n",
       "        0.19036584,  0.19030825,  0.19025069,  0.1901929 ,  0.19013546,\n",
       "        0.19007811,  0.190021  ,  0.18996378,  0.18990663,  0.18984948,\n",
       "        0.18979239,  0.18973508,  0.18967835,  0.18962176,  0.18956498,\n",
       "        0.18950829,  0.18945174,  0.1893952 ,  0.18933893,  0.18928254,\n",
       "        0.18922592,  0.18916958,  0.1891132 ,  0.18905688,  0.18900056,\n",
       "        0.18894438,  0.18888827,  0.18883221,  0.18877591,  0.18872008,\n",
       "        0.18866444,  0.18860884,  0.18855333,  0.18849778,  0.18844228,\n",
       "        0.18838683,  0.18833118,  0.18827584,  0.18822039,  0.18816498,\n",
       "        0.18810964,  0.18805427,  0.18799912,  0.18794356,  0.18788837,\n",
       "        0.18783328,  0.18777815,  0.1877245 ,  0.18767155,  0.18761777,\n",
       "        0.18756492,  0.18751129,  0.18745772,  0.18740425,  0.18735074,\n",
       "        0.18729897,  0.18724726,  0.1871957 ,  0.18714252,  0.18709099,\n",
       "        0.18703751,  0.18698429,  0.18693146,  0.18687864,  0.18682681,\n",
       "        0.18677405,  0.1867217 ,  0.18666851,  0.18661661,  0.1865651 ,\n",
       "        0.18651335,  0.1864624 ,  0.1864109 ,  0.18635947,  0.18630827,\n",
       "        0.18625698,  0.18620718,  0.18615745,  0.18610784,  0.1860582 ,\n",
       "        0.18600867,  0.1859592 ,  0.1859098 ,  0.18586044,  0.18580935,\n",
       "        0.18575906,  0.18571002,  0.18565877,  0.18560865,  0.18555783,\n",
       "        0.18550712,  0.18545762,  0.18540707,  0.18535654,  0.18530614,\n",
       "        0.18525587,  0.18520572,  0.18515554,  0.18510514,  0.18505479,\n",
       "        0.18500512,  0.18495548,  0.18490535,  0.18485585,  0.18480673,\n",
       "        0.18475767,  0.18470861,  0.18465964,  0.1846107 ,  0.18456219,\n",
       "        0.18451465,  0.18446714,  0.18441938,  0.18437171,  0.18432409,\n",
       "        0.18427652,  0.18422902,  0.18417991,  0.18413083,  0.18408182,\n",
       "        0.18403432,  0.18398689,  0.18393952,  0.18389218,  0.18384281,\n",
       "        0.18379376,  0.18374478,  0.18369585,  0.18364702,  0.18359925,\n",
       "        0.18355124,  0.18350329,  0.18345539,  0.1834076 ,  0.18335985,\n",
       "        0.18331217,  0.18326456,  0.18321947,  0.18317449,  0.18312707,\n",
       "        0.18307972,  0.18303452,  0.18298969,  0.18294497,  0.18290031,\n",
       "        0.1828557 ,  0.18281105,  0.18276527,  0.18271828,  0.18267272,\n",
       "        0.18262721,  0.18258131,  0.18253548,  0.1824897 ,  0.18244431,\n",
       "        0.18239901,  0.18235367,  0.18230859,  0.18226356,  0.18221698,\n",
       "        0.18217048,  0.182124  ,  0.18207761,  0.18203277,  0.18198798,\n",
       "        0.18194327,  0.18189852,  0.18185386,  0.18180926,  0.18176472,\n",
       "        0.18171938,  0.18167371,  0.1816285 ,  0.18158334,  0.18153823,\n",
       "        0.18149315,  0.18144751,  0.18140192,  0.18135749,  0.18131311,\n",
       "        0.18126826,  0.18122362,  0.1811798 ,  0.18113603,  0.18109156,\n",
       "        0.18104713,  0.18100296,  0.18095883,  0.18091476,  0.18087075,\n",
       "        0.18082696,  0.18078308,  0.18073927,  0.18069591,  0.18065258,\n",
       "        0.1806093 ,  0.18056606,  0.1805229 ,  0.18047901,  0.18043515,\n",
       "        0.18039134,  0.18034759,  0.18030389,  0.18026033,  0.18021748,\n",
       "        0.18017404,  0.18013068,  0.18008763,  0.18004464,  0.18000171,\n",
       "        0.17995882,  0.17991572,  0.17987293,  0.17982991,  0.17978734,\n",
       "        0.17974482,  0.17970236,  0.17965995,  0.17961722,  0.17957454,\n",
       "        0.17953191,  0.17948933,  0.17944681,  0.17940437,  0.17936198,\n",
       "        0.17931966,  0.17927742,  0.17923521,  0.17919305,  0.17915097,\n",
       "        0.17910863,  0.17906637,  0.17902417,  0.17898203,  0.17893947,\n",
       "        0.17889722,  0.17885525,  0.17881336,  0.17877153,  0.17872976])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.train_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.2594           0.0001           22.71s\n",
      "         2           0.2721           0.0001           20.78s\n",
      "         3           0.2560           0.0001           19.98s\n",
      "         4           0.2617           0.0001           19.60s\n",
      "         5           0.2419           0.0001           19.24s\n",
      "         6           0.2652           0.0001           19.10s\n",
      "         7           0.2617           0.0001           18.94s\n",
      "         8           0.2515           0.0001           18.82s      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.2603           0.0001            4.03m         1           0.2424           0.0001            4.37m         1           0.2492           0.0000            4.69m\n",
      "\n",
      "\n",
      "         2           0.2643           0.0001            3.41m         2           0.2698           0.0001            3.68m         2           0.2668           0.0001            4.06m\n",
      "\n",
      "\n",
      "         3           0.2624           0.0001            3.11m\n",
      "         3           0.2623           0.0001            3.31m\n",
      "         3           0.2562           0.0000            3.40m\n",
      "         4           0.2661           0.0001            2.85m\n",
      "         4           0.2602           0.0001            3.02m\n",
      "         4           0.2540           0.0001            3.15m\n",
      "         5           0.2770           0.0001            2.70m\n",
      "         5           0.2809           0.0001            2.78m\n",
      "         5           0.2498           0.0001            2.90m\n",
      "         6           0.2583           0.0001            2.49m\n",
      "         6           0.2426           0.0001            2.55m\n",
      "         6           0.2440           0.0001            2.62m\n",
      "         7           0.2480           0.0001            2.26m\n",
      "         7           0.2652           0.0001            2.34m\n",
      "         7           0.2595           0.0001            2.41m\n",
      "         8           0.2798           0.0001            2.07m\n",
      "         8           0.2643           0.0001            2.12m\n",
      "         8           0.2328           0.0001            2.21m\n",
      "         9           0.2458           0.0001            1.92m\n",
      "         9           0.2443           0.0001            1.97m\n",
      "         9           0.2235           0.0001            2.04m\n",
      "        10           0.2438           0.0001            1.78m\n",
      "        10           0.2632           0.0001            1.81m\n",
      "        10           0.2463           0.0001            1.89m\n",
      "        20           0.2568           0.0001            1.09m        20           0.2574           0.0001            1.10m        20           0.2387           0.0001            1.14m\n",
      "\n",
      "\n",
      "        30           0.2452           0.0001           49.84s        30           0.2658           0.0001           49.92s        30           0.2759           0.0001           52.05s\n",
      "\n",
      "\n",
      "        40           0.2292           0.0001           42.21s        40           0.2349           0.0001           41.57s        40           0.2224           0.0001           43.28s\n",
      "\n",
      "\n",
      "        50           0.2319           0.0001           36.93s        50           0.2518           0.0001           36.37s        50           0.2806           0.0001           37.77s\n",
      "\n",
      "\n",
      "        60           0.2677           0.0001           33.47s        60           0.2765           0.0001           32.87s        60           0.2747           0.0001           34.10s\n",
      "\n",
      "\n",
      "        70           0.2506           0.0001           30.98s        70           0.2724           0.0001           30.40s        70           0.2750           0.0001           31.49s\n",
      "\n",
      "\n",
      "        80           0.2391           0.0001           29.09s        80           0.2459           0.0001           28.44s        80           0.2398           0.0001           29.48s\n",
      "\n",
      "\n",
      "        90           0.2389           0.0001           27.50s        90           0.2486           0.0001           26.92s        90           0.2445           0.0001           27.87s\n",
      "\n",
      "\n",
      "       100           0.2363           0.0001           26.26s       100           0.2485           0.0001           25.73s       100           0.2604           0.0001           26.57s\n",
      "\n",
      "\n",
      "       200           0.2320           0.0001           19.38s       200           0.2478           0.0001           18.96s       200           0.2288           0.0001           19.59s\n",
      "\n",
      "\n",
      "       300           0.2336           0.0001           16.68s       300           0.2245           0.0001           16.28s       300           0.2299           0.0001           16.67s\n",
      "\n",
      "\n",
      "       400           0.2167           0.0001           14.77s       400           0.2574           0.0001           14.52s       400           0.2372           0.0000           14.83s\n",
      "\n",
      "\n",
      "       500           0.2240           0.0001           13.44s       500           0.2299           0.0000           13.30s       500           0.2216           0.0001           13.54s\n",
      "\n",
      "\n",
      "       600           0.2178           0.0001           12.47s       600           0.2149           0.0001           12.32s       600           0.1888           0.0000           12.55s\n",
      "\n",
      "\n",
      "       700           0.2122           0.0001           11.61s       700           0.2155           0.0000           11.52s       700           0.2002           0.0000           11.89s\n",
      "\n",
      "\n",
      "       800           0.1879           0.0000           10.93s       800           0.2051           0.0000           10.82s       800           0.1985           0.0000           11.50s\n",
      "\n",
      "\n",
      "       900           0.1952           0.0000           10.54s       900           0.2045           0.0000           10.22s       900           0.1978           0.0000           10.81s\n",
      "\n",
      "\n",
      "      1000           0.1969           0.0000            9.89s      1000           0.2120           0.0000            9.61s      1000           0.2100           0.0000           10.17s\n",
      "\n",
      "\n",
      "      2000           0.1696           0.0000            4.64s      2000           0.1772           0.0000            4.50s      2000           0.1463           0.0000            4.82s\n",
      "\n",
      "\n",
      "      3000           0.1683          -0.0000            0.00s      3000           0.1746           0.0000            0.00s      3000           0.1585          -0.0000            0.00s\n",
      "\n",
      "\n",
      "\n",
      "         9           0.2501           0.0001           20.99s\n",
      "        10           0.2576           0.0001           20.94s\n",
      "        20           0.2599           0.0001           19.35s\n",
      "        30           0.2590           0.0001           18.83s\n",
      "        40           0.2582           0.0001           18.56s\n",
      "        50           0.2629           0.0001           18.36s\n",
      "        60           0.2527           0.0001           18.21s\n",
      "        70           0.2508           0.0001           18.07s\n",
      "        80           0.2268           0.0001           17.95s\n",
      "        90           0.2503           0.0001           17.84s\n",
      "       100           0.2516           0.0001           17.76s\n",
      "       200           0.2468           0.0001           16.90s\n",
      "       300           0.2473           0.0001           16.15s\n",
      "       400           0.2192           0.0001           15.50s\n",
      "       500           0.2157           0.0000           14.86s\n",
      "       600           0.2202           0.0000           14.25s\n",
      "       700           0.2028           0.0000           13.64s\n",
      "       800           0.2062           0.0000           13.04s\n",
      "       900           0.2052           0.0000           12.45s\n",
      "      1000           0.2221           0.0000           11.85s\n",
      "      2000           0.1641           0.0000            5.91s\n",
      "      3000           0.1727           0.0000            0.00s\n",
      "{'max_features': 0.5, 'subsample': 0.15, 'learning_rate': 0.001, 'max_depth': 4, 'min_samples_leaf': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done   3 out of   3 | elapsed:   14.1s finished\n"
     ]
    }
   ],
   "source": [
    "# set grid search param\n",
    "param_grid = {'learning_rate': [0.001],\n",
    "              'max_depth': [4],\n",
    "              'min_samples_leaf': [9],\n",
    "              'max_features': [0.5],\n",
    "              'subsample': [0.15]\n",
    "              }\n",
    "\n",
    "# init regressor\n",
    "regressor = GradientBoostingRegressor(n_estimators=3000,\n",
    "                                      subsample=0.15,\n",
    "                                      loss='lad',\n",
    "                                      verbose=1)\n",
    "\n",
    "# grid search\n",
    "grid_searcher = GridSearchCV(regressor, param_grid, verbose=1, n_jobs=36)\n",
    "grid_searcher.fit(all_xs[:3000], all_ys[:3000])\n",
    "\n",
    "# print best params\n",
    "opt_params = grid_searcher.best_params_\n",
    "print(opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.2720           0.0001           22.67s\n",
      "         2           0.2563           0.0001           21.62s\n",
      "         3           0.2434           0.0001           20.89s\n",
      "         4           0.2508           0.0001           20.54s\n",
      "         5           0.2444           0.0001           20.39s\n",
      "         6           0.2494           0.0001           20.19s\n",
      "         7           0.2637           0.0001           20.01s\n",
      "         8           0.2623           0.0001           19.90s\n",
      "         9           0.2647           0.0001           19.93s\n",
      "        10           0.2458           0.0001           19.76s\n",
      "        20           0.2513           0.0001           19.00s\n",
      "        30           0.2695           0.0001           18.56s\n",
      "        40           0.2419           0.0001           18.25s\n",
      "        50           0.2714           0.0001           18.03s\n",
      "        60           0.2498           0.0001           17.86s\n",
      "        70           0.2669           0.0001           17.74s\n",
      "        80           0.2493           0.0001           17.62s\n",
      "        90           0.2532           0.0001           17.52s\n",
      "       100           0.2507           0.0001           17.42s\n",
      "       200           0.2378           0.0001           16.65s\n",
      "       300           0.2365           0.0001           15.99s\n",
      "       400           0.2123           0.0001           15.35s\n",
      "       500           0.2138           0.0001           14.74s\n",
      "       600           0.2066           0.0001           14.15s\n",
      "       700           0.2219           0.0000           13.54s\n",
      "       800           0.2058           0.0000           12.93s\n",
      "       900           0.1918           0.0000           12.33s\n",
      "      1000           0.1890           0.0000           11.73s\n",
      "      2000           0.1815           0.0000            5.85s\n",
      "      3000           0.1664           0.0000            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
       "             loss='lad', max_depth=4, max_features=0.5,\n",
       "             max_leaf_nodes=None, min_samples_leaf=9, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=3000,\n",
       "             presort='auto', random_state=None, subsample=0.15, verbose=1,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_regressor = GradientBoostingRegressor(n_estimators=3000, loss='lad', verbose=1,\n",
    "                                          learning_rate    = opt_params['learning_rate'],\n",
    "                                          max_depth        = opt_params['max_depth'],\n",
    "                                          min_samples_leaf = opt_params['min_samples_leaf'],\n",
    "                                          max_features     = opt_params['max_features'],\n",
    "                                          subsample        = opt_params['subsample']\n",
    "                                          )\n",
    "opt_regressor.fit(all_xs[:3000], all_ys[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset prepared\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(opt_regressor, \n",
    "            open(\"opt_regressor_%s_%s_%s_%s_%s.pickle\" % (3000,\n",
    "                                                          opt_params['max_depth'],\n",
    "                                                          opt_params['min_samples_leaf'],\n",
    "                                                          opt_params['max_features'],\n",
    "                                                          opt_params['subsample']),\n",
    "                 \"wb\"),\n",
    "            protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt_regressor = pickle.load(open('opt_regressor_3000_4_9_0.5_0.15.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elapsed time: 1635.42340994\n"
     ]
    }
   ],
   "source": [
    "########## Produce Test ##########\n",
    "\n",
    "# load helpful_data.json\n",
    "test_data = pickle.load(open('helpful_data.pickle', 'rb'))\n",
    "\n",
    "# on test set\n",
    "test_helpfuls_predict = [\n",
    "    predict_helpful(d, opt_regressor.predict) for d in test_data]\n",
    "\n",
    "# load 'pairs_Helpful.txt'\n",
    "# get header_str and user_item_outofs\n",
    "with open('pairs_Helpful.txt') as f:\n",
    "    # read and strip lines\n",
    "    lines = [l.strip() for l in f.readlines()]\n",
    "    # stirip out the headers\n",
    "    header_str = lines.pop(0)\n",
    "    # get a list of user_item_ids\n",
    "    user_item_outofs = [l.split('-') for l in lines]\n",
    "    user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "\n",
    "# make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "    assert d['reviewerID'] == user_id\n",
    "    assert d['itemID'] == item_id\n",
    "    assert d['helpful']['outOf'] == outof\n",
    "\n",
    "# write to output file\n",
    "f = open('predictions_Helpful.txt', 'w')\n",
    "print(header_str, file=f)\n",
    "for (user_id, item_id, outof), helpful_predict in zip(user_item_outofs,\n",
    "                                                      test_helpfuls_predict):\n",
    "    print('%s-%s-%s,%s' %\n",
    "          (user_id, item_id, int(outof), round(helpful_predict)), file=f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('total elapsed time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
