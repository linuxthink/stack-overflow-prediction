{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Maintain this file, s.t. maximally similar to the helpful_grid_search.py\n",
    "\n",
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.mean(np.fabs(helpfuls_predict - helpfuls.astype(float)))\n",
    "\n",
    "# load pre computed features\n",
    "global_feature, users_feature, items_feature = pickle.load(\n",
    "    open('global_users_items_feature.feature', 'rb'))\n",
    "style_dict = pickle.load(open('style_dict.feature', 'rb'))\n",
    "\n",
    "# feature engineering\n",
    "def get_feature_time(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(\n",
    "        unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = float(y)\n",
    "    m = float(m)\n",
    "    d = float(d)\n",
    "    return [y, m, d]\n",
    "\n",
    "def get_feature_style(d):\n",
    "    # load from style dict\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    s = style_dict[user_id][item_id]\n",
    "\n",
    "    feature = [s['num_words'],\n",
    "               s['num_words_summary'],\n",
    "               s['redability'],\n",
    "               s['avg_word_len'],\n",
    "               s['num_words'] /\n",
    "               s['num_sentences'] if s['num_sentences'] != 0.0 else 0.0,\n",
    "               s['num_unique_words'],\n",
    "               s['exclam_exclam_count'] + s['question_count'],\n",
    "               s['dotdotdot_count'],\n",
    "               s['capital_ratio']\n",
    "               ]\n",
    "    return feature\n",
    "\n",
    "def get_time_spot_ratio(times, spot):\n",
    "    # return the array index ratio to insert spot\n",
    "    if len(times) == 0:\n",
    "        return 0.\n",
    "    index = np.searchsorted(np.array(times), spot)\n",
    "    return float(index) / float(len(times))\n",
    "\n",
    "def get_feature_user(d):\n",
    "    user_id = d['reviewerID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "\n",
    "    s = users_feature[user_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "               ]\n",
    "    return feature\n",
    "\n",
    "def get_feature_item(d):\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "\n",
    "    s = items_feature[item_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "               ]\n",
    "    return feature\n",
    "\n",
    "def get_feature(d):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "\n",
    "    # offset\n",
    "    feature = [1.0]\n",
    "\n",
    "    # user\n",
    "    feature += get_feature_user(d)\n",
    "    # item\n",
    "    feature += get_feature_item(d)\n",
    "\n",
    "    # outof\n",
    "    feature += [float(d['helpful']['outOf'])]\n",
    "    # rating\n",
    "    feature += [float(d['rating'])]\n",
    "    # styles\n",
    "    feature += get_feature_style(d)\n",
    "    # time\n",
    "    feature += get_feature_time(d)\n",
    "\n",
    "    return feature\n",
    "\n",
    "# get [feature, label] from single datum\n",
    "def get_feature_label_weight(d, total_outof_weights):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    assert outof != 0.\n",
    "\n",
    "    # feature\n",
    "    feature = get_feature(d)\n",
    "    # label\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / \\\n",
    "        float(d['helpful']['outOf'])\n",
    "    # weight\n",
    "    weight = float(d['helpful']['outOf']) / total_outof_weights\n",
    "\n",
    "    return (feature, ratio_label, weight)\n",
    "\n",
    "# build [feature, label] list from entire dataset\n",
    "def make_dataset(train_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    weights = []\n",
    "\n",
    "    train_outofs = np.array([d['helpful']['outOf']\n",
    "                             for d in train_data]).astype(float)\n",
    "    total_outof_weights = np.sum(train_outofs)\n",
    "\n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label, weight = get_feature_label_weight(\n",
    "            d, total_outof_weights)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "\n",
    "    return (np.array(features), np.array(labels), np.array(weights))\n",
    "\n",
    "# make one prediction\n",
    "def predict_helpful(d, ratio_predictor):\n",
    "    # ratio_predictor[func]: y = ratio_predictor(get_feature(d))\n",
    "\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "\n",
    "    if (user_id in users_feature) and (item_id in items_feature):\n",
    "        predict = ratio_predictor(np.array(get_feature(d)).reshape((1, -1)))\n",
    "        ratio = predict[0]  # np.ndarray\n",
    "    elif (user_id in users_feature) and (item_id not in items_feature):\n",
    "        ratio = users_feature[user_id]['ratio_b']\n",
    "    elif (user_id not in users_feature) and (item_id in items_feature):\n",
    "        ratio = items_ratio[item_id]['ratio_b']\n",
    "    else:\n",
    "        ratio = global_feature['global_ratio_b']\n",
    "    return ratio * outof\n",
    "\n",
    "# make predictions and get mae on a dataset\n",
    "def get_valid_mae(valid_data, ratio_predictor):\n",
    "    # ground truth nhelpful\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    # predited nhelpful\n",
    "    helpfuls_predict = np.array(\n",
    "        [predict_helpful(d, ratio_predictor) for d in valid_data])\n",
    "    # return mae\n",
    "    return get_mae(helpfuls, helpfuls_predict)\n",
    "\n",
    "##########  Grid Search ##########\n",
    "\n",
    "opt_regressor_name = 'opt_0.01_4_9_0.5_0.1'\n",
    "opt_regressor = pickle.load(open(opt_regressor_name + '.pickle', 'rb'))\n",
    "\n",
    "########## Produce Test ##########\n",
    "\n",
    "# load helpful_data.json\n",
    "test_data = pickle.load(open('helpful_data.pickle', 'rb'))\n",
    "\n",
    "# on test set\n",
    "test_helpfuls_predict = [\n",
    "    predict_helpful(d, opt_regressor.predict) for d in test_data]\n",
    "\n",
    "# load 'pairs_Helpful.txt'\n",
    "# get header_str and user_item_outofs\n",
    "with open('pairs_Helpful.txt') as f:\n",
    "    # read and strip lines\n",
    "    lines = [l.strip() for l in f.readlines()]\n",
    "    # stirip out the headers\n",
    "    header_str = lines.pop(0)\n",
    "    # get a list of user_item_ids\n",
    "    user_item_outofs = [l.split('-') for l in lines]\n",
    "    user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "\n",
    "# make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "    assert d['reviewerID'] == user_id\n",
    "    assert d['itemID'] == item_id\n",
    "    assert d['helpful']['outOf'] == outof\n",
    "\n",
    "# write to output file\n",
    "f = open('predictions_Helpful.txt', 'w')\n",
    "print(header_str, file=f)\n",
    "for (user_id, item_id, outof), helpful_predict in zip(user_item_outofs,\n",
    "                                                      test_helpfuls_predict):\n",
    "    print('%s-%s-%s,%s' %\n",
    "          (user_id, item_id, int(outof), round(helpful_predict)), file=f)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
