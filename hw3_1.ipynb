{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import os\n",
    "from __future__ import print_function\n",
    "\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_size = len(all_data)\n",
    "train_size = 900000\n",
    "train_data = all_data[:train_size]\n",
    "valid_data = all_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datum = train_data[0]\n",
    "print(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_abs_error(helpfuls, helpfuls_predict):\n",
    "    return np.sum(np.abs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.1 alpha as global mean of helpful ratio\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "alpha = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "print('avg helpfulness ratio', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.2 mean absolute error in validation set\n",
    "valid_helpfuls = np.array([d['helpful']['nHelpful'] for d in valid_data])\n",
    "valid_outofs =  np.array([d['helpful']['outOf'] for d in valid_data])\n",
    "valid_helpfuls_predict = valid_outofs * alpha\n",
    "print('mean_abs_error', get_mean_abs_error(valid_helpfuls, valid_helpfuls_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.3 fit model\n",
    "# theta[0] + theta[1] * #words_in_review + theta[2] * review_star_rating\n",
    "\n",
    "###### WARING !!! WRONG SINCE SOME ENTRY IS REMOVED !!! #######\n",
    "def get_feature(d):\n",
    "    num_review_words = len(d['reviewText'].split())\n",
    "    star = d['rating']\n",
    "    return [1.0, num_review_words, star]\n",
    "\n",
    "def get_ratio(d):\n",
    "    return float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "\n",
    "# linear regression, ignore devided by zero data\n",
    "########## WARNING !!!! Modify here to fit either train data or all data\n",
    "# train_xs = np.array([get_feature(d) for d in train_data if d['helpful']['outOf'] != 0])\n",
    "# train_ys = np.array([get_ratio(d) for d in train_data if d['helpful']['outOf'] != 0])\n",
    "\n",
    "train_xs = np.array([get_feature(d) for d in all_data if d['helpful']['outOf'] != 0])\n",
    "train_ys = np.array([get_ratio(d) for d in all_data if d['helpful']['outOf'] != 0])\n",
    "\n",
    "theta, residuals, rank, s = np.linalg.lstsq(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on train set\n",
    "train_features = np.array([get_feature(d) for d in train_data])\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs = np.array([d['helpful']['outOf'] for d in train_data])\n",
    "train_helpfuls_predict = np.dot(train_features, theta) * train_outofs\n",
    "print('mae', get_mean_abs_error(train_helpfuls, train_helpfuls_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# on valid set\n",
    "valid_features = np.array([get_feature(d) for d in valid_data])\n",
    "valid_helpfuls = np.array([d['helpful']['nHelpful'] for d in valid_data])\n",
    "valid_outofs = np.array([d['helpful']['outOf'] for d in valid_data])\n",
    "valid_helpfuls_predict = np.dot(valid_features, theta) * valid_outofs\n",
    "print('mae', get_mean_abs_error(valid_helpfuls, valid_helpfuls_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load helpful_data.json\n",
    "test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "\n",
    "# on test set\n",
    "test_features = np.array([get_feature(d) for d in test_data])\n",
    "test_outofs = np.array([d['helpful']['outOf'] for d in test_data])\n",
    "test_helpfuls_predict = np.dot(test_features, theta) * test_outofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load 'pairs_Helpful.txt'\n",
    "# get header_str and user_item_outofs\n",
    "with open('pairs_Helpful.txt') as f:\n",
    "    # read and strip lines\n",
    "    lines = [l.strip() for l in f.readlines()]\n",
    "    # stirip out the headers\n",
    "    header_str = lines.pop(0)\n",
    "    # get a list of user_item_ids\n",
    "    user_item_outofs = [l.split('-') for l in lines]\n",
    "    user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "    \n",
    "# make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "    assert d['reviewerID'] == user_id\n",
    "    assert d['itemID'] == item_id\n",
    "    assert d['helpful']['outOf'] == outof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to output file\n",
    "f = open('predictions_Helpful.txt', 'w')\n",
    "print(header_str, file=f)\n",
    "for (user_id, item_id, outof), test_helpful_predict in zip(user_item_outofs, test_helpfuls_predict):\n",
    "    print('%s-%s-%s,%s' % (user_id, item_id, int(outof), test_helpful_predict), file=f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
