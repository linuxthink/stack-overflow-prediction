{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# l1-norm\n",
    "import cvxopt as co\n",
    "from l1 import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6738989353\n"
     ]
    }
   ],
   "source": [
    "# load all_data and test_data\n",
    "start_time = time.time()\n",
    "all_data = pickle.load(open(\"all_data.pickle\", \"rb\"))\n",
    "test_data = pickle.load(open(\"helpful_data.pickle\", \"rb\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(all_data)\n",
    "train_data = all_data[:900000]\n",
    "valid_data = all_data[900000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "for i in reversed(range(len(train_data))):\n",
    "    d = train_data[i] \n",
    "    if d['helpful']['outOf'] > 3000:\n",
    "        train_data.pop(i)\n",
    "    elif d['helpful']['outOf'] < d['helpful']['nHelpful']:\n",
    "        train_data.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.sum(np.fabs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_review_length': 204.28218857142858,\n",
      " 'avg_summary_length': 4.6970666666666663,\n",
      " 'global_ratio_a': 0.76854905454294209,\n",
      " 'global_ratio_b': 0.73847302637676648,\n",
      " 'global_ratio_c': 0.85700000000000043}\n"
     ]
    }
   ],
   "source": [
    "# global feature\n",
    "global_feature = dict()\n",
    "\n",
    "# ratio a: global average\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data]).astype(float)\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data]).astype(float)\n",
    "global_ratio_a = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "\n",
    "# ratio b: average of individual ratios\n",
    "pure_helpfuls = np.copy(train_helpfuls)\n",
    "pure_outofs = np.copy(train_outofs)\n",
    "pure_helpfuls = pure_helpfuls[pure_outofs != 0]\n",
    "pure_outofs = pure_outofs[pure_outofs != 0]\n",
    "global_ratio_b = np.mean(pure_helpfuls / pure_outofs)\n",
    "\n",
    "# ratio c: linear search best ratio\n",
    "def linear_search_ratio(helpfuls, outofs, search_range=(0.3, 1.0, 0.001)):\n",
    "    alphas = np.arange(*search_range)\n",
    "    errors = [get_mae(helpfuls, outofs * alpha) for alpha in alphas]\n",
    "    optimal_alpha = alphas[np.argmin(errors)]\n",
    "    return optimal_alpha\n",
    "\n",
    "global_ratio_c = linear_search_ratio(train_helpfuls, train_outofs, search_range=(0.3, 1.0, 0.001))\n",
    "\n",
    "# avg review length and summary length\n",
    "avg_review_length = np.mean([float(len(d['reviewText'].split())) for d in all_data + test_data])\n",
    "avg_summary_length = np.mean([float(len(d['summary'].split())) for d in all_data + test_data])\n",
    "\n",
    "global_feature['global_ratio_a'] = global_ratio_a\n",
    "global_feature['global_ratio_b'] = global_ratio_b\n",
    "global_feature['global_ratio_c'] = global_ratio_c\n",
    "global_feature['avg_review_length'] = avg_review_length\n",
    "global_feature['avg_summary_length'] = avg_summary_length\n",
    "pprint(global_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_reviews': 247.0, 'avg_review_length': 229.72064777327935, 'ratio_b': 0.67760470431248399, 'ratio_a': 0.74827586206896557, 'review_times': [1311897600, 1314662400, 1317513600, 1317513600, 1317513600, 1317513600, 1317513600, 1317513600, 1317513600, 1319673600, 1322697600, 1322697600, 1322697600, 1322697600, 1322697600, 1322697600, 1322697600, 1323561600, 1324598400, 1325376000, 1325808000, 1328054400, 1328054400, 1328054400, 1330128000, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1331424000, 1331596800, 1332115200, 1332201600, 1332201600, 1332288000, 1333238400, 1334361600, 1334620800, 1334707200, 1334793600, 1335139200, 1335225600, 1335225600, 1335398400, 1336089600, 1336089600, 1336521600, 1336521600, 1337040000, 1337299200, 1338422400, 1338422400, 1338854400, 1340668800, 1340668800, 1340928000, 1340928000, 1340928000, 1340928000, 1340928000, 1340928000, 1343692800, 1343692800, 1343692800, 1343692800, 1343865600, 1343865600, 1344124800, 1344470400, 1344902400, 1345507200, 1345593600, 1345593600, 1346025600, 1346112000, 1346112000, 1347235200, 1347235200, 1347580800, 1348531200, 1348790400, 1348963200, 1349049600, 1349049600, 1349049600, 1351555200, 1351555200, 1351555200, 1351641600, 1351641600, 1351641600, 1351814400, 1352246400, 1352678400, 1353456000, 1354320000, 1354320000, 1355011200, 1355184000, 1355270400, 1356220800, 1356739200, 1357603200, 1357689600, 1357776000, 1358380800, 1358726400, 1358812800, 1359158400, 1359417600, 1359936000, 1360022400, 1360281600, 1361491200, 1361664000, 1361664000, 1361750400, 1361836800, 1361836800, 1361836800, 1362009600, 1362009600, 1362096000, 1362441600, 1362614400, 1362614400, 1363564800, 1363910400, 1364256000, 1364256000, 1365033600, 1365897600, 1365897600, 1366070400, 1366588800, 1366761600, 1366848000, 1367193600, 1367280000, 1367280000, 1367798400, 1368576000, 1369094400, 1369180800, 1369699200, 1370390400, 1370390400, 1370476800, 1370563200, 1370736000, 1370908800, 1370995200, 1371081600, 1371081600, 1371081600, 1371600000, 1371600000, 1372118400, 1372118400, 1372118400, 1372464000, 1372550400, 1372723200, 1373068800, 1373328000, 1373846400, 1374192000, 1374537600, 1374710400, 1375401600, 1375747200, 1375920000, 1376265600, 1376352000, 1376870400, 1376956800, 1376956800, 1377302400, 1377475200, 1377561600, 1377561600, 1377993600, 1377993600, 1378166400, 1378166400, 1380240000, 1380499200, 1380844800, 1381536000, 1381708800, 1381795200, 1381968000, 1382400000, 1382659200, 1383004800, 1383004800, 1383264000, 1383609600, 1383696000, 1383782400, 1384905600, 1386201600, 1386979200, 1386979200, 1387324800, 1387411200, 1387411200, 1387843200, 1388102400, 1388361600, 1388448000, 1388448000, 1388966400, 1388966400, 1389139200, 1389139200, 1389744000, 1390176000, 1390348800, 1393286400, 1393372800, 1393459200, 1393632000, 1393718400, 1393804800, 1394496000, 1395100800, 1396915200, 1397520000, 1398038400, 1398643200, 1398643200, 1398643200, 1398643200, 1398816000, 1399248000, 1400198400, 1400198400, 1401235200, 1402444800, 1402444800, 1402963200, 1403740800], 'avg_summary_length': 3.4453441295546559}\n"
     ]
    }
   ],
   "source": [
    "# user feature\n",
    "user_ids = list(set([d['reviewerID'] for d in all_data + test_data]))\n",
    "users_feature = dict()\n",
    "for user_id in user_ids:\n",
    "    users_feature[user_id] = dict()\n",
    "\n",
    "# 1. compute ratios\n",
    "users_outofs = defaultdict(list)\n",
    "users_helpfuls = defaultdict(list)\n",
    "\n",
    "for d in train_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_helpfuls[user_id].append(float(d['helpful']['nHelpful']))\n",
    "    users_outofs[user_id].append(float(d['helpful']['outOf']))\n",
    "\n",
    "# ratio_a\n",
    "for user_id in users_outofs:\n",
    "    if np.sum(users_outofs[user_id]) != 0:\n",
    "        users_feature[user_id]['ratio_a'] = np.sum(users_helpfuls[user_id]) / np.sum(users_outofs[user_id])\n",
    "    else:\n",
    "        users_feature[user_id]['ratio_a'] = global_feature['global_ratio_a']\n",
    "        \n",
    "# ratio_b\n",
    "for user_id in users_outofs:\n",
    "    if np.sum(users_outofs[user_id]) != 0:\n",
    "        helpfuls = np.array(users_helpfuls[user_id])\n",
    "        outofs = np.array(users_outofs[user_id])\n",
    "        # remove zero outofs\n",
    "        helpfuls = helpfuls[outofs != 0]\n",
    "        outofs = outofs[outofs != 0]\n",
    "        # ratios\n",
    "        ratios = helpfuls / outofs\n",
    "        users_feature[user_id]['ratio_b'] = np.mean(ratios)\n",
    "    else:\n",
    "        users_feature[user_id]['ratio_b'] = global_feature['global_ratio_b']\n",
    "        \n",
    "# 2. number of reviews by the user\n",
    "users_num_review = defaultdict(float)\n",
    "for d in all_data + test_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_num_review[user_id] += 1.0\n",
    "for user_id in users_feature:\n",
    "    users_feature[user_id]['num_reviews'] = users_num_review[user_id]\n",
    "    \n",
    "# 3. time line ratio and time spot ratio (store all review_times for usage)\n",
    "users_review_times = defaultdict(list)\n",
    "for d in all_data + test_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_review_times[user_id].append(d['unixReviewTime'])\n",
    "    users_review_times[user_id] = sorted(users_review_times[user_id])\n",
    "for user_id in users_feature:\n",
    "    users_feature[user_id]['review_times'] = users_review_times[user_id]\n",
    "    # np.searchsorted(users_feature['U805524026']['review_times'], 1387324800) / float(len(users_feature['U805524026']['review_times']))\n",
    "    \n",
    "# 4. average review length and summary length by the user\n",
    "users_review_lengths = defaultdict(list)\n",
    "users_summary_lengths = defaultdict(list)\n",
    "for d in all_data + test_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_review_lengths[user_id].append(float(len(d['reviewText'].split())))\n",
    "    users_summary_lengths[user_id].append(float(len(d['summary'].split())))\n",
    "\n",
    "for user_id in users_feature:\n",
    "    if users_feature[user_id]['num_reviews'] == 0:\n",
    "        users_feature[user_id]['avg_review_length'] = global_feature['avg_review_length']\n",
    "        users_feature[user_id]['avg_summary_length'] = global_feature['avg_summary_length']\n",
    "    else:\n",
    "        assert len(users_review_lengths[user_id]) > 0\n",
    "        assert len(users_summary_lengths[user_id]) > 0\n",
    "        users_feature[user_id]['avg_review_length'] = np.mean(users_review_lengths[user_id])\n",
    "        users_feature[user_id]['avg_summary_length'] = np.mean(users_summary_lengths[user_id])\n",
    "\n",
    "print(users_feature['U805524026'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collect = [d for d in train_data + test_data if d['reviewerID'] == 'U805524026']\n",
    "# print(len(collect))\n",
    "# avg_review_lengths = np.mean([float(len(d['reviewText'].split())) for d in collect])\n",
    "# avg_summary_lengths = np.mean([float(len(d['summary'].split())) for d in collect])\n",
    "# print(avg_review_lengths, avg_summary_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_reviews': 24.0, 'avg_review_length': 91.166666666666671, 'ratio_b': 0.83333333333333337, 'ratio_a': 0.72727272727272729, 'review_times': [1336521600, 1359158400, 1359676800, 1359676800, 1359676800, 1359763200, 1359936000, 1361491200, 1361577600, 1362009600, 1362355200, 1363219200, 1363737600, 1365811200, 1368576000, 1371945600, 1376956800, 1377820800, 1389744000, 1392854400, 1395273600, 1396828800, 1398643200, 1402963200], 'avg_summary_length': 3.25}\n"
     ]
    }
   ],
   "source": [
    "# item feature\n",
    "item_ids = list(set([d['itemID'] for d in all_data + test_data]))\n",
    "items_feature = dict()\n",
    "for item_id in item_ids:\n",
    "    items_feature[item_id] = dict()\n",
    "\n",
    "# 1. compute ratios\n",
    "items_outofs = defaultdict(list)\n",
    "items_helpfuls = defaultdict(list)\n",
    "\n",
    "for d in train_data:\n",
    "    item_id = d['itemID']\n",
    "    items_helpfuls[item_id].append(float(d['helpful']['nHelpful']))\n",
    "    items_outofs[item_id].append(float(d['helpful']['outOf']))\n",
    "\n",
    "# ratio_a\n",
    "for item_id in items_outofs:\n",
    "    if np.sum(items_outofs[item_id]) != 0:\n",
    "        items_feature[item_id]['ratio_a'] = np.sum(items_helpfuls[item_id]) / np.sum(items_outofs[item_id])\n",
    "    else:\n",
    "        items_feature[item_id]['ratio_a'] = global_feature['global_ratio_a']\n",
    "\n",
    "# ratio_b\n",
    "for item_id in items_outofs:\n",
    "    if np.sum(items_outofs[item_id]) != 0:\n",
    "        helpfuls = np.array(items_helpfuls[item_id])\n",
    "        outofs = np.array(items_outofs[item_id])\n",
    "        # remove zero outofs\n",
    "        helpfuls = helpfuls[outofs != 0]\n",
    "        outofs = outofs[outofs != 0]\n",
    "        # ratios\n",
    "        ratios = helpfuls / outofs\n",
    "        items_feature[item_id]['ratio_b'] = np.mean(ratios)\n",
    "    else:\n",
    "        items_feature[item_id]['ratio_b'] = global_feature['global_ratio_b']\n",
    "\n",
    "# 2. number of reviews to the item\n",
    "items_num_review = defaultdict(float)\n",
    "for d in all_data + test_data:\n",
    "    item_id = d['itemID']\n",
    "    items_num_review[item_id] += 1.0\n",
    "for item_id in items_feature:\n",
    "    items_feature[item_id]['num_reviews'] = items_num_review[item_id]\n",
    "\n",
    "# 3. time line ratio and time spot ratio (store all review_times for usage)\n",
    "items_review_times = defaultdict(list)\n",
    "for d in all_data + test_data:\n",
    "    item_id = d['itemID']\n",
    "    items_review_times[item_id].append(d['unixReviewTime'])\n",
    "    items_review_times[item_id] = sorted(items_review_times[item_id])\n",
    "for item_id in items_feature:\n",
    "    items_feature[item_id]['review_times'] = items_review_times[item_id]\n",
    "    \n",
    "# 4. average review length and summary length for the item\n",
    "items_review_lengths = defaultdict(list)\n",
    "items_summary_lengths = defaultdict(list)\n",
    "for d in all_data + test_data:\n",
    "    item_id = d['itemID']\n",
    "    items_review_lengths[item_id].append(float(len(d['reviewText'].split())))\n",
    "    items_summary_lengths[item_id].append(float(len(d['summary'].split())))\n",
    "\n",
    "for item_id in items_feature:\n",
    "    if items_feature[item_id]['num_reviews'] == 0:\n",
    "        items_feature[item_id]['avg_review_length'] = global_feature['avg_review_length']\n",
    "        items_feature[item_id]['avg_summary_length'] = global_feature['avg_summary_length']\n",
    "    else:\n",
    "        assert len(items_review_lengths[item_id]) > 0\n",
    "        assert len(items_summary_lengths[item_id]) > 0\n",
    "        items_feature[item_id]['avg_review_length'] = np.mean(items_review_lengths[item_id])\n",
    "        items_feature[item_id]['avg_summary_length'] = np.mean(items_summary_lengths[item_id])\n",
    "\n",
    "print(items_feature['I063511736'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((global_feature, users_feature, items_feature), \n",
    "             open(\"global_users_items_feature.feature\", \"wb\"), \n",
    "             protocol = pickle.HIGHEST_PROTOCOL)\n",
    "global_feature, users_feature, items_feature = pickle.load(open(\"global_users_items_feature.feature\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-computed features\n",
    "with open('style_dict.feature') as f:\n",
    "    style_dict = pickle.load(f)\n",
    "    # style_dict['U243261361']['I572782694']\n",
    "    # {'avg_word_len': 4.857142857142857,\n",
    "    #  'capital_count': 11.0,\n",
    "    #  'capital_ratio': 0.028205128205128206,\n",
    "    #  'dotdotdot_count': 4.0,\n",
    "    #  'exclam_count': 0.0,\n",
    "    #  'exclam_exclam_count': 0.0,\n",
    "    #  'num_chars': 369.0,\n",
    "    #  'num_sentences': 3.0,\n",
    "    #  'num_unique_words': 50,\n",
    "    #  'num_words': 63.0,\n",
    "    #  'num_words_summary': 2,\n",
    "    #  'punctuation_count': 21.0,\n",
    "    #  'punctuation_ratio': 0.05384615384615385,\n",
    "    #  'question_count': 0.0,\n",
    "    #  'redability': 16.65714285714285}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "def get_feature_time(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = float(y)\n",
    "    m = float(m)\n",
    "    d = float(d)\n",
    "    return [y, m, d]\n",
    "\n",
    "def get_feature_style(d):\n",
    "    # load from style dict\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    s = style_dict[user_id][item_id]\n",
    "    \n",
    "    feature = [s['num_words'],\n",
    "               s['num_words_summary'],\n",
    "               s['redability'],\n",
    "               s['avg_word_len'],\n",
    "               s['num_words'] / s['num_sentences'] if s['num_sentences'] != 0.0 else 0.0,\n",
    "               s['num_unique_words'],\n",
    "               s['exclam_exclam_count'] + s['question_count'],\n",
    "               s['dotdotdot_count'],\n",
    "               s['capital_ratio']\n",
    "              ]\n",
    "        \n",
    "    return feature\n",
    "\n",
    "def get_time_spot_ratio(times, spot):\n",
    "    # return the array index ratio to insert spot\n",
    "    if len(times) == 0:\n",
    "        return 0.\n",
    "    index = np.searchsorted(np.array(times), spot)\n",
    "    return float(index) / float(len(times))\n",
    "\n",
    "def get_feature_user(d):\n",
    "    user_id = d['reviewerID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "    \n",
    "    s = users_feature[user_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "              ]\n",
    "    return feature\n",
    "\n",
    "def get_feature_item(d):\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "    \n",
    "    s = items_feature[item_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "              ]\n",
    "    return feature\n",
    "    \n",
    "def get_feature(d):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "    \n",
    "    # offset\n",
    "    feature = [1.0]\n",
    "    \n",
    "    # user\n",
    "    feature += get_feature_user(d)\n",
    "    # item\n",
    "    feature += get_feature_item(d)\n",
    "    \n",
    "    # outof\n",
    "    feature += [float(d['helpful']['outOf'])]\n",
    "    # rating\n",
    "    feature += [float(d['rating'])]\n",
    "    # styles\n",
    "    feature += get_feature_style(d)\n",
    "    # time\n",
    "    feature += get_feature_time(d)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get [feature, label] from single datum\n",
    "def get_feature_label_weight(d, total_outof_weights):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    assert outof != 0.\n",
    "    \n",
    "    # feature\n",
    "    feature = get_feature(d)\n",
    "    # label\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "    # weight\n",
    "    weight = float(d['helpful']['outOf']) / total_outof_weights\n",
    "    \n",
    "    return (feature, ratio_label, weight)\n",
    "\n",
    "# build [feature, label] list from entire dataset\n",
    "def make_dataset(train_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    weights = []\n",
    "    \n",
    "    train_outofs =  np.array([d['helpful']['outOf'] for d in train_data]).astype(float)\n",
    "    total_outof_weights = np.sum(train_outofs)\n",
    "    \n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label, weight = get_feature_label_weight(d, total_outof_weights)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        weights.append(weight)\n",
    "        \n",
    "    return (np.array(features), np.array(labels), np.array(weights))\n",
    "\n",
    "# make one prediction\n",
    "def predict_helpful(d, ratio_predictor):\n",
    "    # ratio_predictor[func]: y = ratio_predictor(get_feature(d))\n",
    "    \n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    \n",
    "    if (user_id in users_feature) and (item_id in items_feature):\n",
    "        predict = ratio_predictor(np.array(get_feature(d)).reshape((1, -1)))\n",
    "        ratio = predict[0] # np.ndarray\n",
    "    elif (user_id in users_feature) and (item_id not in items_feature):\n",
    "        ratio = users_feature[user_id]['ratio_b']\n",
    "    elif (user_id not in users_feature) and (item_id in items_feature):\n",
    "        ratio = items_ratio[item_id]['ratio_b']\n",
    "    else:\n",
    "        ratio = global_feature['global_ratio_b']\n",
    "    return ratio * outof\n",
    "\n",
    "# make predictions and get mae on a dataset\n",
    "def get_valid_mae(valid_data, ratio_predictor):\n",
    "    # ground truth nhelpful\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    # predited nhelpful\n",
    "    helpfuls_predict = np.array([predict_helpful(d, ratio_predictor) for d in valid_data])\n",
    "    # return mae\n",
    "    return get_mae(helpfuls, helpfuls_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build dataset\n",
    "train_xs, train_ys, train_weights = make_dataset(train_data)\n",
    "valid_xs, valid_ys, valid_weights = make_dataset(valid_data)\n",
    "# all_xs, all_ys, all_weights = make_dataset(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697528681083\n"
     ]
    }
   ],
   "source": [
    "# 2-norm linear regression problem\n",
    "class RegressorTwoNorm():\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        self.resitudals = None\n",
    "        self.residuals = None\n",
    "        self.s = None\n",
    "        \n",
    "    def fit(self, xs, ys):\n",
    "        self.theta, self.residuals, self.rank, self.s = np.linalg.lstsq(xs, ys)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return np.array(np.dot(x, self.theta)).reshape((-1,))\n",
    "\n",
    "regressor_two_norm = RegressorTwoNorm()\n",
    "regressor_two_norm.fit(train_xs, train_ys)\n",
    "print(get_valid_mae(valid_data, regressor_two_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697528681082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor_linear = LinearRegression()\n",
    "regressor_linear.fit(train_xs, train_ys)\n",
    "print(get_valid_mae(valid_data, regressor_linear.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  1.0539e+05  2.7942e+04  8e+04  1e-17  3e-06  1e+00\n",
      " 1:  1.0539e+05  2.7942e+04  8e+04  2e-15  3e-06  1e+00\n",
      " 2:  1.0539e+05  2.7942e+04  8e+04  2e-15  3e-06  1e+00\n",
      " 3:  1.0539e+05  2.7942e+04  8e+04  2e-15  3e-06  1e+00\n",
      " 4:  1.0539e+05  2.7943e+04  8e+04  2e-15  3e-06  1e+00\n",
      " 5:  1.0539e+05  2.7945e+04  8e+04  2e-15  3e-06  1e+00\n",
      " 6:  1.0540e+05  2.7952e+04  8e+04  2e-15  3e-06  1e+00\n",
      " 7:  1.0541e+05  2.7966e+04  8e+04  3e-15  3e-06  1e+00\n",
      " 8:  1.0548e+05  2.8035e+04  8e+04  2e-15  3e-06  1e+00\n",
      " 9:  1.0565e+05  2.8224e+04  8e+04  3e-15  3e-06  1e+00\n",
      "10:  1.0644e+05  2.9203e+04  8e+04  2e-15  3e-06  1e+00\n",
      "11:  1.0878e+05  3.2676e+04  8e+04  2e-15  3e-06  9e-01\n",
      "12:  1.1118e+05  4.1315e+04  7e+04  2e-15  3e-06  8e-01\n",
      "13:  1.1039e+05  5.5554e+04  5e+04  2e-15  2e-06  6e-01\n",
      "14:  1.0790e+05  6.7008e+04  4e+04  2e-15  2e-06  4e-01\n",
      "15:  1.0553e+05  7.4618e+04  3e+04  2e-15  1e-06  3e-01\n",
      "16:  1.0277e+05  8.0513e+04  2e+04  2e-15  1e-06  2e-01\n",
      "17:  1.0066e+05  8.4424e+04  2e+04  2e-15  7e-07  1e-01\n",
      "18:  9.8923e+04  8.7251e+04  1e+04  2e-15  5e-07  8e-02\n",
      "19:  9.7455e+04  8.9482e+04  8e+03  2e-15  3e-07  5e-02\n",
      "20:  9.6465e+04  9.0931e+04  6e+03  2e-15  2e-07  3e-02\n",
      "21:  9.5525e+04  9.2202e+04  3e+03  2e-15  1e-07  2e-02\n",
      "22:  9.5019e+04  9.2871e+04  2e+03  2e-15  9e-08  1e-02\n",
      "23:  9.4504e+04  9.3508e+04  1e+03  2e-15  4e-08  5e-03\n",
      "24:  9.4265e+04  9.3794e+04  5e+02  2e-15  2e-08  2e-03\n",
      "25:  9.4130e+04  9.3948e+04  2e+02  2e-15  8e-09  6e-04\n",
      "26:  9.4065e+04  9.4020e+04  5e+01  2e-15  2e-09  1e-04\n",
      "27:  9.4051e+04  9.4036e+04  1e+01  2e-15  6e-10  3e-05\n",
      "28:  9.4047e+04  9.4041e+04  6e+00  2e-15  3e-10  1e-05\n",
      "29:  9.4045e+04  9.4043e+04  2e+00  2e-15  8e-11  4e-06\n",
      "30:  9.4044e+04  9.4043e+04  4e-01  2e-15  2e-11  6e-07\n",
      "31:  9.4044e+04  9.4044e+04  9e-02  2e-15  1e-11  2e-07\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "# 1-norm predictor\n",
    "class RegressorOneNorm():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, xs, ys):\n",
    "        self.P = co.matrix(xs)\n",
    "        self.q = co.matrix(ys.reshape((ys.shape[0], 1)))\n",
    "        self.u = l1(self.P, self.q)\n",
    "        self.theta = np.array(self.u).reshape((-1,))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return np.array(np.dot(x, self.theta)).reshape((-1,))\n",
    "\n",
    "regressor_one_norm = RegressorOneNorm()\n",
    "regressor_one_norm.fit(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.646877687544\n"
     ]
    }
   ],
   "source": [
    "print(get_valid_mae(valid_data, regressor_one_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.001,\n",
       "             loss='lad', max_depth=6, max_features=None,\n",
       "             max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "regressor_gb = GradientBoostingRegressor(learning_rate=0.001, n_estimators=1000, max_depth=6, loss='lad')\n",
    "regressor_gb.fit(train_xs[:10000], train_ys[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563437088378\n"
     ]
    }
   ],
   "source": [
    "print(get_valid_mae(valid_data, regressor_gb.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############ produce test ############\n",
    "\n",
    "# # load helpful_data.json\n",
    "# test_data = pickle.load(open(\"helpful_data.pickle\", \"rb\"))\n",
    "\n",
    "# # on test set\n",
    "# test_helpfuls_predict = [predict_helpful(d, regressor_gb.predict, train_avg_ratio, users_ratio, items_ratio) for d in test_data]\n",
    "\n",
    "# # load 'pairs_Helpful.txt'\n",
    "# # get header_str and user_item_outofs\n",
    "# with open('pairs_Helpful.txt') as f:\n",
    "#     # read and strip lines\n",
    "#     lines = [l.strip() for l in f.readlines()]\n",
    "#     # stirip out the headers\n",
    "#     header_str = lines.pop(0)\n",
    "#     # get a list of user_item_ids\n",
    "#     user_item_outofs = [l.split('-') for l in lines]\n",
    "#     user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "    \n",
    "# # make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "# for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "#     assert d['reviewerID'] == user_id\n",
    "#     assert d['itemID'] == item_id\n",
    "#     assert d['helpful']['outOf'] == outof\n",
    "    \n",
    "# # write to output file\n",
    "# f = open('predictions_Helpful.txt', 'w')\n",
    "# print(header_str, file=f)\n",
    "# for (user_id, item_id, outof), test_helpful_predict in zip(user_item_outofs, test_helpfuls_predict):\n",
    "#     print('%s-%s-%s,%s' % (user_id, item_id, int(outof), test_helpful_predict), file=f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_data = pickle.load(open(\"helpful_data.pickle\", \"rb\"))\n",
    "# outofs = sorted([d['helpful']['outOf'] for d in test_data])\n",
    "# outofs[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
