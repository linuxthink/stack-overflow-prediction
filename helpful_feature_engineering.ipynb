{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# data\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.8979709148\n"
     ]
    }
   ],
   "source": [
    "# load all_data and test_data\n",
    "start_time = time.time()\n",
    "all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "print(time.time() - start_time)\n",
    "# set train data to all data\n",
    "train_data = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "for i in reversed(range(len(train_data))):\n",
    "    d = train_data[i] \n",
    "    if d['helpful']['outOf'] > 3000:\n",
    "        train_data.pop(i)\n",
    "    elif d['helpful']['outOf'] < d['helpful']['nHelpful']:\n",
    "        train_data.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': [['Books']],\n",
       " 'helpful': {'nHelpful': 3, 'outOf': 6},\n",
       " 'itemID': 'I063511736',\n",
       " 'rating': 4.0,\n",
       " 'reviewText': \"where she belongs by Cindy Procter-Kingjess is back home to help her ailing mother. She's comfortable with friends in town and her quiet life.Adam love and works in Destiny Falls.Their lives have once again crossed and they renew their friendship and it grows as time goes on.Jess only planned to stay a month then she would go back to her job in the city but the lure of the logging community and people who live in the area have special appeal to her.\",\n",
       " 'reviewTime': '05 9, 2012',\n",
       " 'reviewerID': 'U805524026',\n",
       " 'summary': 'great',\n",
       " 'unixReviewTime': 1336521600}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.sum(np.fabs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_review_length': 204.27815053300557,\n",
      " 'avg_summary_length': 4.6970453976184512,\n",
      " 'global_ratio_a': 0.76806459007747785,\n",
      " 'global_ratio_b': 0.73842172208868673,\n",
      " 'global_ratio_c': 0.85500000000000043}\n"
     ]
    }
   ],
   "source": [
    "# global feature\n",
    "global_feature = dict()\n",
    "\n",
    "# ratio a: global average\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data]).astype(float)\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data]).astype(float)\n",
    "global_ratio_a = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "\n",
    "# ratio b: average of individual ratios\n",
    "pure_helpfuls = np.copy(train_helpfuls)\n",
    "pure_outofs = np.copy(train_outofs)\n",
    "pure_helpfuls = pure_helpfuls[pure_outofs != 0]\n",
    "pure_outofs = pure_outofs[pure_outofs != 0]\n",
    "global_ratio_b = np.mean(pure_helpfuls / pure_outofs)\n",
    "\n",
    "# ratio c: linear search best ratio\n",
    "def linear_search_ratio(helpfuls, outofs, search_range=(0.3, 1.0, 0.001)):\n",
    "    alphas = np.arange(*search_range)\n",
    "    errors = [get_mae(helpfuls, outofs * alpha) for alpha in alphas]\n",
    "    optimal_alpha = alphas[np.argmin(errors)]\n",
    "    return optimal_alpha\n",
    "\n",
    "global_ratio_c = linear_search_ratio(train_helpfuls, train_outofs, search_range=(0.3, 1.0, 0.001))\n",
    "\n",
    "# avg review length and summary length\n",
    "avg_review_length = np.mean([float(len(d['reviewText'].split())) for d in train_data + test_data])\n",
    "avg_summary_length = np.mean([float(len(d['summary'].split())) for d in train_data + test_data])\n",
    "\n",
    "global_feature['global_ratio_a'] = global_ratio_a\n",
    "global_feature['global_ratio_b'] = global_ratio_b\n",
    "global_feature['global_ratio_c'] = global_ratio_c\n",
    "global_feature['avg_review_length'] = avg_review_length\n",
    "global_feature['avg_summary_length'] = avg_summary_length\n",
    "pprint(global_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_reviews': 247.0, 'avg_review_length': 229.72064777327935, 'ratio_b': 0.69224987624812895, 'ratio_a': 0.76000000000000001, 'review_times': [1311897600, 1314662400, 1317513600, 1317513600, 1317513600, 1317513600, 1317513600, 1317513600, 1317513600, 1319673600, 1322697600, 1322697600, 1322697600, 1322697600, 1322697600, 1322697600, 1322697600, 1323561600, 1324598400, 1325376000, 1325808000, 1328054400, 1328054400, 1328054400, 1330128000, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1330473600, 1331424000, 1331596800, 1332115200, 1332201600, 1332201600, 1332288000, 1333238400, 1334361600, 1334620800, 1334707200, 1334793600, 1335139200, 1335225600, 1335225600, 1335398400, 1336089600, 1336089600, 1336521600, 1336521600, 1337040000, 1337299200, 1338422400, 1338422400, 1338854400, 1340668800, 1340668800, 1340928000, 1340928000, 1340928000, 1340928000, 1340928000, 1340928000, 1343692800, 1343692800, 1343692800, 1343692800, 1343865600, 1343865600, 1344124800, 1344470400, 1344902400, 1345507200, 1345593600, 1345593600, 1346025600, 1346112000, 1346112000, 1347235200, 1347235200, 1347580800, 1348531200, 1348790400, 1348963200, 1349049600, 1349049600, 1349049600, 1351555200, 1351555200, 1351555200, 1351641600, 1351641600, 1351641600, 1351814400, 1352246400, 1352678400, 1353456000, 1354320000, 1354320000, 1355011200, 1355184000, 1355270400, 1356220800, 1356739200, 1357603200, 1357689600, 1357776000, 1358380800, 1358726400, 1358812800, 1359158400, 1359417600, 1359936000, 1360022400, 1360281600, 1361491200, 1361664000, 1361664000, 1361750400, 1361836800, 1361836800, 1361836800, 1362009600, 1362009600, 1362096000, 1362441600, 1362614400, 1362614400, 1363564800, 1363910400, 1364256000, 1364256000, 1365033600, 1365897600, 1365897600, 1366070400, 1366588800, 1366761600, 1366848000, 1367193600, 1367280000, 1367280000, 1367798400, 1368576000, 1369094400, 1369180800, 1369699200, 1370390400, 1370390400, 1370476800, 1370563200, 1370736000, 1370908800, 1370995200, 1371081600, 1371081600, 1371081600, 1371600000, 1371600000, 1372118400, 1372118400, 1372118400, 1372464000, 1372550400, 1372723200, 1373068800, 1373328000, 1373846400, 1374192000, 1374537600, 1374710400, 1375401600, 1375747200, 1375920000, 1376265600, 1376352000, 1376870400, 1376956800, 1376956800, 1377302400, 1377475200, 1377561600, 1377561600, 1377993600, 1377993600, 1378166400, 1378166400, 1380240000, 1380499200, 1380844800, 1381536000, 1381708800, 1381795200, 1381968000, 1382400000, 1382659200, 1383004800, 1383004800, 1383264000, 1383609600, 1383696000, 1383782400, 1384905600, 1386201600, 1386979200, 1386979200, 1387324800, 1387411200, 1387411200, 1387843200, 1388102400, 1388361600, 1388448000, 1388448000, 1388966400, 1388966400, 1389139200, 1389139200, 1389744000, 1390176000, 1390348800, 1393286400, 1393372800, 1393459200, 1393632000, 1393718400, 1393804800, 1394496000, 1395100800, 1396915200, 1397520000, 1398038400, 1398643200, 1398643200, 1398643200, 1398643200, 1398816000, 1399248000, 1400198400, 1400198400, 1401235200, 1402444800, 1402444800, 1402963200, 1403740800], 'avg_summary_length': 3.4453441295546559}\n"
     ]
    }
   ],
   "source": [
    "# user feature\n",
    "user_ids = list(set([d['reviewerID'] for d in train_data + test_data]))\n",
    "users_feature = dict()\n",
    "for user_id in user_ids:\n",
    "    users_feature[user_id] = dict()\n",
    "\n",
    "# 1. compute ratios\n",
    "users_outofs = defaultdict(list)\n",
    "users_helpfuls = defaultdict(list)\n",
    "\n",
    "for d in train_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_helpfuls[user_id].append(float(d['helpful']['nHelpful']))\n",
    "    users_outofs[user_id].append(float(d['helpful']['outOf']))\n",
    "\n",
    "# ratio_a\n",
    "for user_id in users_outofs:\n",
    "    if np.sum(users_outofs[user_id]) != 0:\n",
    "        users_feature[user_id]['ratio_a'] = np.sum(users_helpfuls[user_id]) / np.sum(users_outofs[user_id])\n",
    "    else:\n",
    "        users_feature[user_id]['ratio_a'] = global_feature['global_ratio_a']\n",
    "        \n",
    "# ratio_b\n",
    "for user_id in users_outofs:\n",
    "    if np.sum(users_outofs[user_id]) != 0:\n",
    "        helpfuls = np.array(users_helpfuls[user_id])\n",
    "        outofs = np.array(users_outofs[user_id])\n",
    "        # remove zero outofs\n",
    "        helpfuls = helpfuls[outofs != 0]\n",
    "        outofs = outofs[outofs != 0]\n",
    "        # ratios\n",
    "        ratios = helpfuls / outofs\n",
    "        users_feature[user_id]['ratio_b'] = np.mean(ratios)\n",
    "    else:\n",
    "        users_feature[user_id]['ratio_b'] = global_feature['global_ratio_b']\n",
    "        \n",
    "# 2. number of reviews by the user\n",
    "users_num_review = defaultdict(float)\n",
    "for d in train_data + test_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_num_review[user_id] += 1.0\n",
    "for user_id in users_feature:\n",
    "    users_feature[user_id]['num_reviews'] = users_num_review[user_id]\n",
    "    \n",
    "# 3. time line ratio and time spot ratio (store all review_times for usage)\n",
    "users_review_times = defaultdict(list)\n",
    "for d in train_data + test_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_review_times[user_id].append(d['unixReviewTime'])\n",
    "    users_review_times[user_id] = sorted(users_review_times[user_id])\n",
    "for user_id in users_feature:\n",
    "    users_feature[user_id]['review_times'] = users_review_times[user_id]\n",
    "    # np.searchsorted(users_feature['U805524026']['review_times'], 1387324800) / float(len(users_feature['U805524026']['review_times']))\n",
    "    \n",
    "# 4. average review length and summary length by the user\n",
    "users_review_lengths = defaultdict(list)\n",
    "users_summary_lengths = defaultdict(list)\n",
    "for d in train_data + test_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_review_lengths[user_id].append(float(len(d['reviewText'].split())))\n",
    "    users_summary_lengths[user_id].append(float(len(d['summary'].split())))\n",
    "\n",
    "for user_id in users_feature:\n",
    "    if users_feature[user_id]['num_reviews'] == 0:\n",
    "        users_feature[user_id]['avg_review_length'] = global_feature['avg_review_length']\n",
    "        users_feature[user_id]['avg_summary_length'] = global_feature['avg_summary_length']\n",
    "    else:\n",
    "        assert len(users_review_lengths[user_id]) > 0\n",
    "        assert len(users_summary_lengths[user_id]) > 0\n",
    "        users_feature[user_id]['avg_review_length'] = np.mean(users_review_lengths[user_id])\n",
    "        users_feature[user_id]['avg_summary_length'] = np.mean(users_summary_lengths[user_id])\n",
    "\n",
    "print(users_feature['U805524026'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "229.720647773 3.44534412955\n"
     ]
    }
   ],
   "source": [
    "collect = [d for d in train_data + test_data if d['reviewerID'] == 'U805524026']\n",
    "print(len(collect))\n",
    "avg_review_lengths = np.mean([float(len(d['reviewText'].split())) for d in collect])\n",
    "avg_summary_lengths = np.mean([float(len(d['summary'].split())) for d in collect])\n",
    "print(avg_review_lengths, avg_summary_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_reviews': 24.0, 'avg_review_length': 91.166666666666671, 'ratio_b': 0.875, 'ratio_a': 0.75, 'review_times': [1336521600, 1359158400, 1359676800, 1359676800, 1359676800, 1359763200, 1359936000, 1361491200, 1361577600, 1362009600, 1362355200, 1363219200, 1363737600, 1365811200, 1368576000, 1371945600, 1376956800, 1377820800, 1389744000, 1392854400, 1395273600, 1396828800, 1398643200, 1402963200], 'avg_summary_length': 3.25}\n"
     ]
    }
   ],
   "source": [
    "# item feature\n",
    "item_ids = list(set([d['itemID'] for d in train_data + test_data]))\n",
    "items_feature = dict()\n",
    "for item_id in item_ids:\n",
    "    items_feature[item_id] = dict()\n",
    "\n",
    "# 1. compute ratios\n",
    "items_outofs = defaultdict(list)\n",
    "items_helpfuls = defaultdict(list)\n",
    "\n",
    "for d in train_data:\n",
    "    item_id = d['itemID']\n",
    "    items_helpfuls[item_id].append(float(d['helpful']['nHelpful']))\n",
    "    items_outofs[item_id].append(float(d['helpful']['outOf']))\n",
    "\n",
    "# ratio_a\n",
    "for item_id in items_outofs:\n",
    "    if np.sum(items_outofs[item_id]) != 0:\n",
    "        items_feature[item_id]['ratio_a'] = np.sum(items_helpfuls[item_id]) / np.sum(items_outofs[item_id])\n",
    "    else:\n",
    "        items_feature[item_id]['ratio_a'] = global_feature['global_ratio_a']\n",
    "\n",
    "# ratio_b\n",
    "for item_id in items_outofs:\n",
    "    if np.sum(items_outofs[item_id]) != 0:\n",
    "        helpfuls = np.array(items_helpfuls[item_id])\n",
    "        outofs = np.array(items_outofs[item_id])\n",
    "        # remove zero outofs\n",
    "        helpfuls = helpfuls[outofs != 0]\n",
    "        outofs = outofs[outofs != 0]\n",
    "        # ratios\n",
    "        ratios = helpfuls / outofs\n",
    "        items_feature[item_id]['ratio_b'] = np.mean(ratios)\n",
    "    else:\n",
    "        items_feature[item_id]['ratio_b'] = global_feature['global_ratio_b']\n",
    "\n",
    "# 2. number of reviews by the item\n",
    "items_num_review = defaultdict(float)\n",
    "for d in train_data + test_data:\n",
    "    item_id = d['itemID']\n",
    "    items_num_review[item_id] += 1.0\n",
    "for item_id in items_feature:\n",
    "    items_feature[item_id]['num_reviews'] = items_num_review[item_id]\n",
    "\n",
    "# 3. time line ratio and time spot ratio (store all review_times for usage)\n",
    "items_review_times = defaultdict(list)\n",
    "for d in train_data + test_data:\n",
    "    item_id = d['itemID']\n",
    "    items_review_times[item_id].append(d['unixReviewTime'])\n",
    "    items_review_times[item_id] = sorted(items_review_times[item_id])\n",
    "for item_id in items_feature:\n",
    "    items_feature[item_id]['review_times'] = items_review_times[item_id]\n",
    "    \n",
    "# 4. average review length and summary length for the item\n",
    "items_review_lengths = defaultdict(list)\n",
    "items_summary_lengths = defaultdict(list)\n",
    "for d in train_data + test_data:\n",
    "    item_id = d['itemID']\n",
    "    items_review_lengths[item_id].append(float(len(d['reviewText'].split())))\n",
    "    items_summary_lengths[item_id].append(float(len(d['summary'].split())))\n",
    "\n",
    "for item_id in items_feature:\n",
    "    if items_feature[item_id]['num_reviews'] == 0:\n",
    "        items_feature[item_id]['avg_review_length'] = global_feature['avg_review_length']\n",
    "        items_feature[item_id]['avg_summary_length'] = global_feature['avg_summary_length']\n",
    "    else:\n",
    "        assert len(items_review_lengths[item_id]) > 0\n",
    "        assert len(items_summary_lengths[item_id]) > 0\n",
    "        items_feature[item_id]['avg_review_length'] = np.mean(items_review_lengths[item_id])\n",
    "        items_feature[item_id]['avg_summary_length'] = np.mean(items_summary_lengths[item_id])\n",
    "\n",
    "print(items_feature['I063511736'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((global_feature, users_feature, items_feature), \n",
    "             open(\"global_users_items_feature.feature\", \"wb\"), \n",
    "             protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-computed features\n",
    "with open(os.path.join(data_root, 'style_dict.feature')) as f:\n",
    "    style_dict = pickle.load(f)\n",
    "    # style_dict['U243261361']['I572782694']\n",
    "    # {'avg_word_len': 4.857142857142857,\n",
    "    #  'capital_count': 11.0,\n",
    "    #  'capital_ratio': 0.028205128205128206,\n",
    "    #  'dotdotdot_count': 4.0,\n",
    "    #  'exclam_count': 0.0,\n",
    "    #  'exclam_exclam_count': 0.0,\n",
    "    #  'num_chars': 369.0,\n",
    "    #  'num_sentences': 3.0,\n",
    "    #  'num_unique_words': 50,\n",
    "    #  'num_words': 63.0,\n",
    "    #  'num_words_summary': 2,\n",
    "    #  'punctuation_count': 21.0,\n",
    "    #  'punctuation_ratio': 0.05384615384615385,\n",
    "    #  'question_count': 0.0,\n",
    "    #  'redability': 16.65714285714285}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "def get_feature_time(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = float(y)\n",
    "    m = float(m)\n",
    "    d = float(d)\n",
    "    return [y, m, d]\n",
    "\n",
    "def get_feature_style(d):\n",
    "    # load from style dict\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    s = style_dict[user_id][item_id]\n",
    "    \n",
    "    feature = [s['num_words'],\n",
    "               s['num_words_summary'],\n",
    "               s['redability'],\n",
    "               s['avg_word_len'],\n",
    "               s['num_words'] / s['num_sentences'],\n",
    "               s['num_unique_words'],\n",
    "               s['exclam_exclam_count'] + s['question_count'],\n",
    "               s['dotdotdot_count'],\n",
    "               s['capital_ratio']\n",
    "              ]    \n",
    "    return feature\n",
    "\n",
    "def get_time_spot_ratio(times, spot):\n",
    "    # return the array index ratio to insert spot\n",
    "    if len(times) == 0:\n",
    "        return 0.\n",
    "    index = np.searchsorted(np.array(times), spot)\n",
    "    return float(index) / float(len(times))\n",
    "\n",
    "def get_feature_user(d):\n",
    "    user_id = d['reviewerID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "    \n",
    "    s = users_feature[user_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "              ]\n",
    "    return feature\n",
    "\n",
    "def get_feature_item(d):\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "    \n",
    "    s = items_feature[item_id]\n",
    "    feature = [s['ratio_a'],\n",
    "               s['ratio_b'],\n",
    "               s['num_reviews'],\n",
    "               s['avg_review_length'],\n",
    "               s['avg_summary_length'],\n",
    "               get_time_spot_ratio(s['review_times'], unix_time)\n",
    "              ]\n",
    "    return feature\n",
    "    \n",
    "def get_feature(d):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    unix_time = d['unixReviewTime']\n",
    "    \n",
    "    # offset\n",
    "    feature = [1.0]\n",
    "    \n",
    "    # user\n",
    "    feature += get_feature_user(d)\n",
    "    # item\n",
    "    feature += get_feature_item(d)\n",
    "    \n",
    "    # rating\n",
    "    feature += [float(d['rating'])]\n",
    "    # styles\n",
    "    feature += get_feature_style(d)\n",
    "    # time\n",
    "    feature += get_feature_time(d)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get [feature, label] from single datum\n",
    "def get_feature_and_ratio_label(d, users_ratio, items_ratio):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    if outof == 0:\n",
    "        raise('out of cannot be 0 for ratio')\n",
    "\n",
    "    # get feature and ratio\n",
    "    feature = get_feature(d)\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "    return (feature, ratio_label)\n",
    "\n",
    "# build [feature, label] list from entire dataset\n",
    "def make_average_regression_dataset(train_data, users_ratio, items_ratio):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label = get_feature_and_ratio_label(d, users_ratio, items_ratio)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    return (np.array(features), np.array(labels))\n",
    "\n",
    "# make one prediction\n",
    "def predict_helpful(d, ratio_predictor, train_avg_ratio, users_ratio, items_ratio):\n",
    "    # ratio_predictor[func]: y = ratio_predictor(get_feature(d))\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    \n",
    "    if (user_id in users_ratio) and (item_id in items_ratio):\n",
    "        # ratio = np.dot(get_feature(d), theta)\n",
    "        predict = ratio_predictor(get_feature(d))\n",
    "        ratio = predict[0] # np.ndarray\n",
    "    elif (user_id in users_ratio) and (item_id not in items_ratio):\n",
    "        ratio = users_ratio[user_id]\n",
    "    elif (user_id not in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = items_ratio[item_id]\n",
    "    else:\n",
    "        ratio = train_avg_ratio\n",
    "    return ratio * outof\n",
    "\n",
    "# make predictions and get mae on a dataset\n",
    "def get_valid_mae(valid_data, ratio_predictor, train_avg_ratio, users_ratio, items_ratio):\n",
    "    # ground truth nhelpful\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    # predited nhelpful\n",
    "    helpfuls_predict = np.array([predict_helpful(d, ratio_predictor, train_avg_ratio, users_ratio, items_ratio) for d in valid_data])\n",
    "    # return mae\n",
    "    return get_mae(helpfuls, helpfuls_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build dataset\n",
    "# train_xs, train_ys = make_average_regression_dataset(train_data, users_ratio, items_ratio)\n",
    "# valid_xs, valid_ys = make_average_regression_dataset(valid_data, users_ratio, items_ratio)\n",
    "all_xs, all_ys = make_average_regression_dataset(all_data, users_ratio, items_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 2-norm linear regression problem\n",
    "# class RegressorTwoNorm():\n",
    "#     def __init__(self):\n",
    "#         self.theta = None\n",
    "#         self.resitudals = None\n",
    "#         self.residuals = None\n",
    "#         self.s = None\n",
    "        \n",
    "#     def fit(self, xs, ys):\n",
    "#         self.theta, self.residuals, self.rank, self.s = np.linalg.lstsq(xs, ys)\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         return np.array(np.dot(x, self.theta)).reshape((-1,))\n",
    "\n",
    "# regressor_two_norm = RegressorTwoNorm()\n",
    "# regressor_two_norm.fit(train_xs, train_ys)\n",
    "# print(get_valid_mae(valid_data, regressor_two_norm, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# regressor_linear = LinearRegression()\n",
    "# regressor_linear.fit(train_xs, train_ys)\n",
    "# print(get_valid_mae(valid_data, regressor_linear.predict, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 1-norm predictor\n",
    "# class RegressorOneNorm():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def fit(self, xs, ys):\n",
    "#         self.P = co.matrix(xs)\n",
    "#         self.q = co.matrix(ys.reshape((ys.shape[0], 1)))\n",
    "#         self.u = l1(self.P, self.q)\n",
    "#         self.theta = np.array(self.u).reshape((-1,))\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         return np.array(np.dot(x, self.theta)).reshape((-1,))\n",
    "\n",
    "# regressor_one_norm = RegressorOneNorm()\n",
    "# regressor_one_norm.fit(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(get_valid_mae(valid_data, regressor_one_norm, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# regressor_rf = RandomForestRegressor()\n",
    "# regressor_rf.fit(train_xs, train_ys)\n",
    "# print(get_valid_mae(valid_data, regressor_rf.predict, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# regressor_gb = GradientBoostingRegressor(learning_rate=0.001, n_estimators=1000, max_depth=6, loss='lad')\n",
    "# regressor_gb.fit(train_xs[:5000], train_ys[:5000])\n",
    "\n",
    "# print(get_valid_mae(valid_data, regressor_gb.predict, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############ produce test ############\n",
    "\n",
    "# # load helpful_data.json\n",
    "# test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "\n",
    "# # on test set\n",
    "# test_helpfuls_predict = [predict_helpful(d, regressor_gb.predict, train_avg_ratio, users_ratio, items_ratio) for d in test_data]\n",
    "\n",
    "# # load 'pairs_Helpful.txt'\n",
    "# # get header_str and user_item_outofs\n",
    "# with open('pairs_Helpful.txt') as f:\n",
    "#     # read and strip lines\n",
    "#     lines = [l.strip() for l in f.readlines()]\n",
    "#     # stirip out the headers\n",
    "#     header_str = lines.pop(0)\n",
    "#     # get a list of user_item_ids\n",
    "#     user_item_outofs = [l.split('-') for l in lines]\n",
    "#     user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "    \n",
    "# # make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "# for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "#     assert d['reviewerID'] == user_id\n",
    "#     assert d['itemID'] == item_id\n",
    "#     assert d['helpful']['outOf'] == outof\n",
    "    \n",
    "# # write to output file\n",
    "# f = open('predictions_Helpful.txt', 'w')\n",
    "# print(header_str, file=f)\n",
    "# for (user_id, item_id, outof), test_helpful_predict in zip(user_item_outofs, test_helpfuls_predict):\n",
    "#     print('%s-%s-%s,%s' % (user_id, item_id, int(outof), test_helpful_predict), file=f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "# outofs = sorted([d['helpful']['outOf'] for d in test_data])\n",
    "# outofs[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
