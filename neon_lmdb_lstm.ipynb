{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "from neon.data import DataIterator, Text, load_text\n",
    "from neon.initializers import Uniform, GlorotUniform\n",
    "from neon.layers import GeneralizedCost, LSTM, Affine, Dropout, LookupTable, RecurrentSum\n",
    "from neon.models import Model\n",
    "from neon.optimizers import Adagrad\n",
    "from neon.transforms import Logistic, Tanh, Softmax, CrossEntropyMulti, Accuracy\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "from neon.util.argparser import NeonArgparser\n",
    "import numpy as np\n",
    "import os\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummy class for arguments\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "\n",
    "# the command line arguments \n",
    "args.backend         = 'gpu'\n",
    "args.batch_size      = 128\n",
    "args.epochs          = 2\n",
    "\n",
    "args.config          = None\n",
    "args.data_dir        = '/home/linuxthink/nervana/data'\n",
    "args.datatype        = np.float32\n",
    "args.device_id       = 0\n",
    "args.evaluation_freq = 1\n",
    "args.history         = 1\n",
    "args.log_thresh      = 40\n",
    "args.logfile         = None\n",
    "args.model_file      = None\n",
    "args.no_progress_bar = False\n",
    "args.output_file     = None\n",
    "args.progress_bar    = True\n",
    "args.rng_seed        = 0\n",
    "args.rounding        = False\n",
    "args.save_path       = None\n",
    "args.serialize       = 0\n",
    "args.verbose         = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = args.epochs\n",
    "\n",
    "# hyperparameters from the reference\n",
    "batch_size = 128\n",
    "clip_gradients = True\n",
    "gradient_limit = 15\n",
    "vocab_size = 20000\n",
    "sentence_length = 100\n",
    "embedding_dim = 128\n",
    "hidden_size = 128\n",
    "reset_cells = True\n",
    "\n",
    "# setup backend\n",
    "be = gen_backend(backend=args.backend,\n",
    "                 batch_size=batch_size,\n",
    "                 rng_seed=args.rng_seed,\n",
    "                 device_id=args.device_id,\n",
    "                 default_dtype=args.datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make dataset\n",
    "path = load_text('imdb', path=args.data_dir)\n",
    "(X_train, y_train), (X_test, y_test), nclass = Text.pad_data(\n",
    "    path, vocab_size=vocab_size, sentence_length=sentence_length)\n",
    "\n",
    "print \"Vocab size - \", vocab_size\n",
    "print \"Sentence Length - \", sentence_length\n",
    "print \"# of train sentences\", X_train.shape[0]\n",
    "print \"# of test sentence\", X_test.shape[0]\n",
    "\n",
    "train_set = DataIterator(X_train, y_train, nclass=2)\n",
    "valid_set = DataIterator(X_test, y_test, nclass=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "init_emb = Uniform(low=-0.1/embedding_dim, high=0.1/embedding_dim)\n",
    "init_glorot = GlorotUniform()\n",
    "\n",
    "# setup network structures\n",
    "layers = [\n",
    "    LookupTable(vocab_size=vocab_size, embedding_dim=embedding_dim, init=init_emb),\n",
    "    LSTM(hidden_size, init_glorot, activation=Tanh(),\n",
    "         gate_activation=Logistic(), reset_cells=True),\n",
    "    RecurrentSum(),\n",
    "    Dropout(keep=0.5),\n",
    "    Affine(2, init_glorot, bias=init_glorot, activation=Softmax())\n",
    "]\n",
    "\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))\n",
    "metric = Accuracy()\n",
    "\n",
    "model = Model(layers=layers)\n",
    "\n",
    "optimizer = Adagrad(learning_rate=0.01, clip_gradients=clip_gradients)\n",
    "\n",
    "callbacks = Callbacks(model, train_set, args, eval_set=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |████████████████████|  157/157  batches, 0.60 cost, 166.47s] [CrossEntropyMulti Loss 0.58, 4.36s]\n",
      "Epoch 1   [Train |████████████████████|  156/156  batches, 0.40 cost, 111.35s] [CrossEntropyMulti Loss 0.54, 2.64s]\n",
      "Test  Accuracy -  [ 83.54000092]\n",
      "Train Accuracy -  [ 92.49500275]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_set,\n",
    "          optimizer=optimizer,\n",
    "          num_epochs=num_epochs,\n",
    "          cost=cost,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "# eval model\n",
    "print \"Test  Accuracy - \", 100 * model.eval(valid_set, metric=metric)\n",
    "print \"Train Accuracy - \", 100 * model.eval(train_set, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neon.layers.layer.Activation at 0x7f109e751cd0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.layers[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n"
     ]
    }
   ],
   "source": [
    "# evaluate a minibatch\n",
    "for x, t in valid_set:\n",
    "    x = model.fprop(x, inference=True)\n",
    "    print(metric(x, t))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<neon.layers.layer.LookupTable at 0x7f109e751b90>,\n",
       " <neon.layers.recurrent.LSTM at 0x7f109e751ad0>,\n",
       " <neon.layers.recurrent.RecurrentSum at 0x7f10ab114790>,\n",
       " <neon.layers.layer.Dropout at 0x7f109e751c10>,\n",
       " <neon.layers.layer.Linear at 0x7f109e751c90>,\n",
       " <neon.layers.layer.Bias at 0x7f10a653cb50>,\n",
       " <neon.layers.layer.Activation at 0x7f109e751cd0>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get layers construction\n",
    "model.layers.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# get a minibatch data\n",
      "(100, 128)\n",
      "(2, 128)\n",
      "# 0 lookup table layer\n",
      "(128, 12800)\n",
      "0.000362119\n",
      "# 1 LSTM\n",
      "(128, 12800)\n",
      "-0.00104521\n",
      "# 2 RecurrentSum\n",
      "(128, 128)\n",
      "-0.104521\n",
      "[[ 0.35116085  0.11722134  0.73177195 ...,  0.31589624  0.5112744\n",
      "   0.38425255]\n",
      " [ 0.28652561  0.19011086 -0.06047176 ...,  0.17256325  0.08469895\n",
      "  -0.09357169]\n",
      " [-0.11168828 -0.14445594 -0.1072502  ..., -0.17631994 -0.10753911\n",
      "  -0.09727415]\n",
      " ..., \n",
      " [-3.34715319 -3.41540289 -3.11425161 ..., -3.36476421 -3.22862697\n",
      "  -3.2796762 ]\n",
      " [-0.21559666 -0.41813296  0.26141584 ..., -0.18262051  0.01051297\n",
      "  -0.04731833]\n",
      " [-0.36492407 -0.22123244 -0.52946991 ..., -0.33033496 -0.37420812\n",
      "  -0.36613423]]\n",
      "# 3 Dropout\n",
      "(128, 128)\n",
      "-0.101294\n",
      "# 4 Linear\n",
      "(2, 128)\n",
      "0.0501066\n",
      "# 5 Bias\n",
      "(2, 128)\n",
      "1.23326\n",
      "# 6 Activation\n",
      "(2, 128)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# get last layer output\n",
    "\n",
    "print(\"# get a minibatch data\")\n",
    "for x, t in valid_set:\n",
    "    break\n",
    "print(x.get().shape)\n",
    "print(t.get().shape)\n",
    "    \n",
    "print(\"# 0 lookup table layer\")\n",
    "x = model.layers.layers[0].fprop(x)\n",
    "print(x.get().shape)\n",
    "print(np.mean(x.get()))\n",
    "\n",
    "print(\"# 1 LSTM\")\n",
    "x = model.layers.layers[1].fprop(x)\n",
    "print(x.get().shape)\n",
    "print(np.mean(x.get()))\n",
    "\n",
    "print(\"# 2 RecurrentSum\")\n",
    "x = model.layers.layers[2].fprop(x)\n",
    "print(x.get().shape)\n",
    "print(np.mean(x.get()))\n",
    "print(x.get())\n",
    "\n",
    "print(\"# 3 Dropout\")\n",
    "x = model.layers.layers[3].fprop(x)\n",
    "print(x.get().shape)\n",
    "print(np.mean(x.get()))\n",
    "\n",
    "print(\"# 4 Linear\")\n",
    "x = model.layers.layers[4].fprop(x)\n",
    "print(x.get().shape)\n",
    "print(np.mean(x.get()))\n",
    "\n",
    "print(\"# 5 Bias\")\n",
    "x = model.layers.layers[5].fprop(x)\n",
    "print(x.get().shape)\n",
    "print(np.mean(x.get()))\n",
    "\n",
    "print(\"# 6 Activation\")\n",
    "x = model.layers.layers[6].fprop(x)\n",
    "print(x.get().shape)\n",
    "print(np.mean(x.get()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
