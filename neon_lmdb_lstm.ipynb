{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "from neon.data import DataIterator, Text, load_text\n",
    "from neon.initializers import Uniform, GlorotUniform\n",
    "from neon.layers import GeneralizedCost, LSTM, Affine, Dropout, LookupTable, RecurrentSum\n",
    "from neon.models import Model\n",
    "from neon.optimizers import Adagrad\n",
    "from neon.transforms import Logistic, Tanh, Softmax, CrossEntropyMulti, Accuracy\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "from neon.util.argparser import NeonArgparser\n",
    "import numpy as np\n",
    "import os\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dummy class for arguments\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "\n",
    "# the command line arguments \n",
    "args.backend         = 'gpu'\n",
    "args.batch_size      = 128\n",
    "args.epochs          = 2\n",
    "\n",
    "args.config          = None\n",
    "args.data_dir        = '/home/linuxthink/nervana/data'\n",
    "args.datatype        = np.float32\n",
    "args.device_id       = 0\n",
    "args.evaluation_freq = 1\n",
    "args.history         = 1\n",
    "args.log_thresh      = 40\n",
    "args.logfile         = None\n",
    "args.model_file      = None\n",
    "args.no_progress_bar = False\n",
    "args.output_file     = None\n",
    "args.progress_bar    = True\n",
    "args.rng_seed        = 0\n",
    "args.rounding        = False\n",
    "args.save_path       = None\n",
    "args.serialize       = 0\n",
    "args.verbose         = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = args.epochs\n",
    "\n",
    "# hyperparameters from the reference\n",
    "batch_size = 128\n",
    "clip_gradients = True\n",
    "gradient_limit = 15\n",
    "vocab_size = 20000\n",
    "sentence_length = 100\n",
    "embedding_dim = 128\n",
    "hidden_size = 128\n",
    "reset_cells = True\n",
    "\n",
    "# setup backend\n",
    "be = gen_backend(backend=args.backend,\n",
    "                 batch_size=batch_size,\n",
    "                 rng_seed=args.rng_seed,\n",
    "                 device_id=args.device_id,\n",
    "                 default_dtype=args.datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make dataset\n",
    "path = load_text('imdb', path=args.data_dir)\n",
    "(X_train, y_train), (X_test, y_test), nclass = Text.pad_data(\n",
    "    path, vocab_size=vocab_size, sentence_length=sentence_length)\n",
    "\n",
    "print \"Vocab size - \", vocab_size\n",
    "print \"Sentence Length - \", sentence_length\n",
    "print \"# of train sentences\", X_train.shape[0]\n",
    "print \"# of test sentence\", X_test.shape[0]\n",
    "\n",
    "train_set = DataIterator(X_train, y_train, nclass=2)\n",
    "valid_set = DataIterator(X_test, y_test, nclass=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "init_emb = Uniform(low=-0.1/embedding_dim, high=0.1/embedding_dim)\n",
    "init_glorot = GlorotUniform()\n",
    "\n",
    "# setup network structures\n",
    "layers = [\n",
    "    LookupTable(vocab_size=vocab_size, embedding_dim=embedding_dim, init=init_emb),\n",
    "    LSTM(hidden_size, init_glorot, activation=Tanh(),\n",
    "         gate_activation=Logistic(), reset_cells=True),\n",
    "    RecurrentSum(),\n",
    "    Dropout(keep=0.5),\n",
    "    Affine(2, init_glorot, bias=init_glorot, activation=Softmax())\n",
    "]\n",
    "\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))\n",
    "metric = Accuracy()\n",
    "\n",
    "model = Model(layers=layers)\n",
    "\n",
    "optimizer = Adagrad(learning_rate=0.01, clip_gradients=clip_gradients)\n",
    "\n",
    "callbacks = Callbacks(model, train_set, args, eval_set=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |████████████████████|  157/157  batches, 0.60 cost, 166.47s] [CrossEntropyMulti Loss 0.58, 4.36s]\n",
      "Epoch 1   [Train |████████████████████|  156/156  batches, 0.40 cost, 111.35s] [CrossEntropyMulti Loss 0.54, 2.64s]\n",
      "Test  Accuracy -  [ 83.54000092]\n",
      "Train Accuracy -  [ 92.49500275]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_set,\n",
    "          optimizer=optimizer,\n",
    "          num_epochs=num_epochs,\n",
    "          cost=cost,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "# eval model\n",
    "print \"Test  Accuracy - \", 100 * model.eval(valid_set, metric=metric)\n",
    "print \"Train Accuracy - \", 100 * model.eval(train_set, metric=metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
