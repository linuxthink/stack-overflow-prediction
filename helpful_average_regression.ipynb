{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'\n",
    "\n",
    "import cvxopt as co\n",
    "from l1 import l1\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load all_data\n",
    "start_time = time.time()\n",
    "all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split training and valid set\n",
    "# all\n",
    "all_size = len(all_data)\n",
    "\n",
    "# train\n",
    "train_size = 900000\n",
    "# train_size = all_size # uncomment this to produce test\n",
    "train_data = all_data[:train_size]\n",
    "\n",
    "# valid\n",
    "valid_size = 100000\n",
    "valid_data = all_data[all_size - valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "# for i in reversed(range(train_size)):\n",
    "#     if train_data[i]['helpful']['outOf'] > 5000:\n",
    "#         train_data.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get global average\n",
    "# train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "# train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "# train_avg_ratio = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "# print('avg helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# linear search best ratio\n",
    "def linear_search_ratio(helpfuls, outofs, search_range=(0.3, 1.0, 0.001)):\n",
    "    alphas = np.arange(*search_range)\n",
    "    errors = [get_mae(helpfuls, outofs * alpha) for alpha in alphas]\n",
    "    optimal_alpha = alphas[np.argmin(errors)]\n",
    "    return optimal_alpha\n",
    "\n",
    "# training set global\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "train_avg_ratio = linear_search_ratio(train_helpfuls, train_outofs, search_range=(0.3, 1.0, 0.001))\n",
    "print('optimal helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# get average for a user\n",
    "users_outof = dict()\n",
    "users_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_outof[user_id] = users_outof.get(user_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    users_helpful[user_id] = users_helpful.get(user_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "    \n",
    "users_ratio = dict()\n",
    "for user_id in users_outof:\n",
    "    if users_outof[user_id] != 0:\n",
    "        users_ratio[user_id] = users_helpful[user_id] / users_outof[user_id]\n",
    "    else:\n",
    "        users_ratio[user_id] = train_avg_ratio\n",
    "        \n",
    "# get average for a item\n",
    "items_outof = dict()\n",
    "items_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    item_id = d['itemID']\n",
    "    items_outof[item_id] = items_outof.get(item_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    items_helpful[item_id] = items_helpful.get(item_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "\n",
    "items_ratio = dict()\n",
    "for item_id in items_outof:\n",
    "    if items_outof[item_id] != 0:\n",
    "        items_ratio[item_id] = items_helpful[item_id] / items_outof[item_id]\n",
    "    else:\n",
    "        items_ratio[item_id] = train_avg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "punction = set(string.punctuation)\n",
    "\n",
    "with open('betas.pickle') as f:\n",
    "    beta_us, beta_is = pickle.load(f)\n",
    "    \n",
    "with open('train_ratio_list.pickle') as f:\n",
    "    train_ratio_list = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root, 'num_unique_word.feature')) as f:\n",
    "    num_unique_word_dict = pickle.load(f)\n",
    "\n",
    "# get date time statistics\n",
    "def get_y_m_d(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = int(y)\n",
    "    m = int(m)\n",
    "    d = int(d)\n",
    "    return(y, m, d)\n",
    "\n",
    "def get_time_feature(d):\n",
    "    y, m, d = get_y_m_d(d)\n",
    "    y = min(y, 2014)\n",
    "    y = max(y, 1996)\n",
    "    # 1996 [1,0,..,0] 2014 [0,0,...,0]\n",
    "    y_feature = [0] * (2014 - 1996 + 1)\n",
    "    y_feature[y - 1996] = 1\n",
    "    # jan [1,0,...,0] dec [0,0,...,0]\n",
    "    m_feature = [0] * 12\n",
    "    m_feature[m - 1] = 1\n",
    "    # date1 [1,0,...,0] date31 [0,0,...,0]\n",
    "    d_feature = [0] * 31\n",
    "    d_feature[d - 1] = 1\n",
    "    # concatenate\n",
    "    feature = y_feature[:-1] + m_feature[:-1] + d_feature[:-1]\n",
    "    return feature\n",
    "\n",
    "def get_num_uique_word(d):\n",
    "    wordCount = defaultdict(int)\n",
    "    for w in d[\"reviewText\"].split():\n",
    "        w = \"\".join([c for c in w.lower() if not c in punction])\n",
    "        w = stemmer.stem(w)\n",
    "        wordCount[w] += 1\n",
    "    return len(wordCount)\n",
    "\n",
    "def get_feature(d):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    feature = [1.0,\n",
    "               num_unique_word_dict[user_id][item_id],\n",
    "               users_ratio[user_id],\n",
    "               items_ratio[item_id],\n",
    "               len(d['reviewText'].split()),\n",
    "               d['rating']]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.sum(np.fabs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]\n",
    "\n",
    "def get_valid_mae(valid_data, theta, train_avg_ratio, users_ratio, items_ratio):\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    helpfuls_predict = np.array([predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio) for d in valid_data])\n",
    "    return get_mae(helpfuls, helpfuls_predict)\n",
    "\n",
    "# get [feature, label] from single datum\n",
    "def get_feature_and_ratio_label(d, users_ratio, items_ratio):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    if outof == 0:\n",
    "        raise('out of cannot be 0 for ratio')\n",
    "\n",
    "    # get feature and ratio\n",
    "    feature = get_feature(d)\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "    return (feature, ratio_label)\n",
    "\n",
    "# build [feature, label] list from entire dataset\n",
    "def make_average_regression_dataset(train_data, users_ratio, items_ratio):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label = get_feature_and_ratio_label(d, users_ratio, items_ratio)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    return (np.array(features), np.array(labels))\n",
    "\n",
    "# the official prediction function\n",
    "def predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    \n",
    "    if (user_id in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = np.dot(get_feature(d), theta)\n",
    "    elif (user_id in users_ratio) and (item_id not in items_ratio):\n",
    "        ratio = users_ratio[user_id]\n",
    "    elif (user_id not in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = items_ratio[item_id]\n",
    "    else:\n",
    "        ratio = train_avg_ratio\n",
    "    return ratio * outof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build dataset\n",
    "train_xs, train_ys = make_average_regression_dataset(train_data, users_ratio, items_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solve 2-norm minimize problem\n",
    "theta, residuals, rank, s = np.linalg.lstsq(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solve 1-norm minimize problem\n",
    "P = co.matrix(train_xs)\n",
    "q = co.matrix(train_ys.reshape((train_ys.shape[0], 1)))\n",
    "u = l1(P,q)\n",
    "theta = np.array(u).reshape((-1,))\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validation result\n",
    "print(get_valid_mae(valid_data, theta, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############ produce test ############\n",
    "\n",
    "# # load helpful_data.json\n",
    "# test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "\n",
    "# # on test set\n",
    "# test_helpfuls_predict = [predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio) for d in test_data]\n",
    "\n",
    "# # load 'pairs_Helpful.txt'\n",
    "# # get header_str and user_item_outofs\n",
    "# with open('pairs_Helpful.txt') as f:\n",
    "#     # read and strip lines\n",
    "#     lines = [l.strip() for l in f.readlines()]\n",
    "#     # stirip out the headers\n",
    "#     header_str = lines.pop(0)\n",
    "#     # get a list of user_item_ids\n",
    "#     user_item_outofs = [l.split('-') for l in lines]\n",
    "#     user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "    \n",
    "# # make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "# for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "#     assert d['reviewerID'] == user_id\n",
    "#     assert d['itemID'] == item_id\n",
    "#     assert d['helpful']['outOf'] == outof\n",
    "    \n",
    "# # write to output file\n",
    "# f = open('predictions_Helpful.txt', 'w')\n",
    "# print(header_str, file=f)\n",
    "# for (user_id, item_id, outof), test_helpful_predict in zip(user_item_outofs, test_helpfuls_predict):\n",
    "#     print('%s-%s-%s,%s' % (user_id, item_id, int(outof), test_helpful_predict), file=f)\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
