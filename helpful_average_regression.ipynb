{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "# data\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'\n",
    "\n",
    "# l1-norm\n",
    "import cvxopt as co\n",
    "from l1 import l1\n",
    "\n",
    "# natural language processing\n",
    "import nltk\n",
    "import nltk.data\n",
    "import string\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.7351691723\n"
     ]
    }
   ],
   "source": [
    "# load all_data\n",
    "start_time = time.time()\n",
    "all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split training and valid set\n",
    "# all\n",
    "all_size = len(all_data)\n",
    "\n",
    "# train\n",
    "train_size = 900000\n",
    "# train_size = all_size # uncomment this to produce test\n",
    "train_data = all_data[:train_size]\n",
    "\n",
    "# valid\n",
    "valid_size = 100000\n",
    "valid_data = all_data[all_size - valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "# for i in reversed(range(train_size)):\n",
    "#     if train_data[i]['helpful']['outOf'] > 5000:\n",
    "#         train_data.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg helpfulness ratio 0.770464430813\n",
      "optimal helpfulness ratio 0.857\n"
     ]
    }
   ],
   "source": [
    "# get global average\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "train_avg_ratio = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "print('avg helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# linear search best ratio\n",
    "def linear_search_ratio(helpfuls, outofs, search_range=(0.3, 1.0, 0.001)):\n",
    "    alphas = np.arange(*search_range)\n",
    "    errors = [get_mae(helpfuls, outofs * alpha) for alpha in alphas]\n",
    "    optimal_alpha = alphas[np.argmin(errors)]\n",
    "    return optimal_alpha\n",
    "\n",
    "# training set global\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "train_avg_ratio = linear_search_ratio(train_helpfuls, train_outofs, search_range=(0.3, 1.0, 0.001))\n",
    "print('optimal helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# get average for a user\n",
    "users_outof = dict()\n",
    "users_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_outof[user_id] = users_outof.get(user_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    users_helpful[user_id] = users_helpful.get(user_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "    \n",
    "users_ratio = dict()\n",
    "for user_id in users_outof:\n",
    "    if users_outof[user_id] != 0:\n",
    "        users_ratio[user_id] = users_helpful[user_id] / users_outof[user_id]\n",
    "    else:\n",
    "        users_ratio[user_id] = train_avg_ratio\n",
    "        \n",
    "# get average for a item\n",
    "items_outof = dict()\n",
    "items_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    item_id = d['itemID']\n",
    "    items_outof[item_id] = items_outof.get(item_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    items_helpful[item_id] = items_helpful.get(item_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "\n",
    "items_ratio = dict()\n",
    "for item_id in items_outof:\n",
    "    if items_outof[item_id] != 0:\n",
    "        items_ratio[item_id] = items_helpful[item_id] / items_outof[item_id]\n",
    "    else:\n",
    "        items_ratio[item_id] = train_avg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': [['Books']],\n",
       " 'helpful': {'nHelpful': 0, 'outOf': 0},\n",
       " 'itemID': 'I572782694',\n",
       " 'rating': 5.0,\n",
       " 'reviewText': 'favorite of the series...May not have been as steamy as some of the others...but the characters, their depth, and believability were amazing.  wanted to curl up with Devlin and make it all better(wink wink). an amazing series...found Laura Kate when I stumbled onto Hearts in Darkness(one of my all time faves)...this series ranks up there with my Kresley Cole and Gena Showalter favorites.',\n",
       " 'reviewTime': '05 3, 2014',\n",
       " 'reviewerID': 'U243261361',\n",
       " 'summary': 'Loved it',\n",
       " 'unixReviewTime': 1399075200}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "with open('betas.pickle') as f:\n",
    "    beta_us, beta_is = pickle.load(f)\n",
    "    \n",
    "with open('train_ratio_list.pickle') as f:\n",
    "    train_ratio_list = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root, 'num_unique_word.feature')) as f:\n",
    "    num_unique_word_dict = pickle.load(f)\n",
    "\n",
    "# get date time statistics\n",
    "def get_y_m_d(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = int(y)\n",
    "    m = int(m)\n",
    "    d = int(d)\n",
    "    return(y, m, d)\n",
    "\n",
    "def get_feature_time(d):\n",
    "    y, m, d = get_y_m_d(d)\n",
    "    y = min(y, 2014)\n",
    "    y = max(y, 1996)\n",
    "    # 1996 [1,0,..,0] 2014 [0,0,...,0]\n",
    "    y_feature = [0] * (2014 - 1996 + 1)\n",
    "    y_feature[y - 1996] = 1\n",
    "    # jan [1,0,...,0] dec [0,0,...,0]\n",
    "    m_feature = [0] * 12\n",
    "    m_feature[m - 1] = 1\n",
    "    # date1 [1,0,...,0] date31 [0,0,...,0]\n",
    "    d_feature = [0] * 31\n",
    "    d_feature[d - 1] = 1\n",
    "    # concatenate\n",
    "    feature = y_feature[:-1] + m_feature[:-1] + d_feature[:-1]\n",
    "    return feature\n",
    "\n",
    "def get_num_uique_word(d):\n",
    "    wordCount = defaultdict(int)\n",
    "    for w in d[\"reviewText\"].split():\n",
    "        w = \"\".join([c for c in w.lower() if not c in punctuation])\n",
    "        w = stemmer.stem(w)\n",
    "        wordCount[w] += 1\n",
    "    return len(wordCount)\n",
    "\n",
    "def get_feature(d):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    \n",
    "    feature = [1.0]\n",
    "    feature += [num_unique_word_dict[user_id][item_id]]\n",
    "    feature += [users_ratio[user_id], items_ratio[item_id]]\n",
    "    feature += [float(d['rating']), abs(float(d['rating']) - 0.770464430813)]\n",
    "    feature += get_feature_style(d)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.sum(np.fabs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]\n",
    "\n",
    "def get_valid_mae(valid_data, theta, train_avg_ratio, users_ratio, items_ratio):\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    helpfuls_predict = np.array([predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio) for d in valid_data])\n",
    "    return get_mae(helpfuls, helpfuls_predict)\n",
    "\n",
    "# get [feature, label] from single datum\n",
    "def get_feature_and_ratio_label(d, users_ratio, items_ratio):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    if outof == 0:\n",
    "        raise('out of cannot be 0 for ratio')\n",
    "\n",
    "    # get feature and ratio\n",
    "    feature = get_feature(d)\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "    return (feature, ratio_label)\n",
    "\n",
    "# build [feature, label] list from entire dataset\n",
    "def make_average_regression_dataset(train_data, users_ratio, items_ratio):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label = get_feature_and_ratio_label(d, users_ratio, items_ratio)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    return (np.array(features), np.array(labels))\n",
    "\n",
    "# the official prediction function\n",
    "def predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    \n",
    "    if (user_id in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = np.dot(get_feature(d), theta)\n",
    "    elif (user_id in users_ratio) and (item_id not in items_ratio):\n",
    "        ratio = users_ratio[user_id]\n",
    "    elif (user_id not in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = items_ratio[item_id]\n",
    "    else:\n",
    "        ratio = train_avg_ratio\n",
    "    return ratio * outof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build dataset\n",
    "train_xs, train_ys = make_average_regression_dataset(train_data, users_ratio, items_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "train_xs_backup = np.zeros_like(train_xs)\n",
    "train_xs_backup[:] = train_xs\n",
    "\n",
    "train_xs = train_xs_backup[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]]\n",
    "\n",
    "# [num_chars, num_words, num_sentences, redability, punctuation_ratio, capital_ratio, avg_word_len, exclam_count, dotdotdot_count]\n",
    "#  6          7          8              9           10                 11             12            13            14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solve 2-norm minimize problem\n",
    "theta, residuals, rank, s = np.linalg.lstsq(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  1.1407e+05  2.9669e+04  8e+04  1e-17  4e+02  1e+00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Rank(A) < p or Rank([G; A]) < n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-0d172cbbf137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mco\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_xs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mco\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/linuxthink/Dropbox/courses/CSE255/assignment1/l1.pyc\u001b[0m in \u001b[0;36ml1\u001b[1;34m(P, q)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'l'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'q'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     sol = solvers.conelp(c, Fi, h, dims, kktsolver = Fkkt,  \n\u001b[1;32m--> 197\u001b[1;33m         primalstart={'x': x0, 's': s0}, dualstart={'z': z0})\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/cvxopt/coneprog.pyc\u001b[0m in \u001b[0;36mconelp\u001b[1;34m(c, G, h, dims, A, b, primalstart, dualstart, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mArithmeticError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miters\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mprimalstart\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdualstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Rank(A) < p or Rank([G; A]) < n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1081\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m                 \u001b[0mxscal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Rank(A) < p or Rank([G; A]) < n"
     ]
    }
   ],
   "source": [
    "# solve 1-norm minimize problem\n",
    "P = co.matrix(train_xs)\n",
    "q = co.matrix(train_ys.reshape((train_ys.shape[0], 1)))\n",
    "u = l1(P,q)\n",
    "theta = np.array(u).reshape((-1,))\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validation result\n",
    "print(get_valid_mae(valid_data, theta, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############ produce test ############\n",
    "\n",
    "# # load helpful_data.json\n",
    "# test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "\n",
    "# # on test set\n",
    "# test_helpfuls_predict = [predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio) for d in test_data]\n",
    "\n",
    "# # load 'pairs_Helpful.txt'\n",
    "# # get header_str and user_item_outofs\n",
    "# with open('pairs_Helpful.txt') as f:\n",
    "#     # read and strip lines\n",
    "#     lines = [l.strip() for l in f.readlines()]\n",
    "#     # stirip out the headers\n",
    "#     header_str = lines.pop(0)\n",
    "#     # get a list of user_item_ids\n",
    "#     user_item_outofs = [l.split('-') for l in lines]\n",
    "#     user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "    \n",
    "# # make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "# for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "#     assert d['reviewerID'] == user_id\n",
    "#     assert d['itemID'] == item_id\n",
    "#     assert d['helpful']['outOf'] == outof\n",
    "    \n",
    "# # write to output file\n",
    "# f = open('predictions_Helpful.txt', 'w')\n",
    "# print(header_str, file=f)\n",
    "# for (user_id, item_id, outof), test_helpful_predict in zip(user_item_outofs, test_helpfuls_predict):\n",
    "#     print('%s-%s-%s,%s' % (user_id, item_id, int(outof), test_helpful_predict), file=f)\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
