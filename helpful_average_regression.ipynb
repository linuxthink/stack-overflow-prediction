{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idea:\n",
    "# - every user, every item, and global have an average\n",
    "# - after averaging statistics, these average is fixed\n",
    "# - fit regression model to fit this average to the original training data to get coefficient\n",
    "# - thus must be better than the provided baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'\n",
    "\n",
    "import cvxopt as co\n",
    "from l1 import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8827280998\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_size = len(all_data)\n",
    "train_size = 900000\n",
    "# train_size = all_size # uncomment this to produce test\n",
    "valid_size = 100000\n",
    "train_data = all_data[:train_size]\n",
    "valid_data = all_data[all_size - valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.sum(np.fabs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]\n",
    "\n",
    "def get_valid_mae(valid_data):\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    helpfuls_predict = np.array([predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio) for d in valid_data])\n",
    "    return get_mae(helpfuls, helpfuls_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg helpfulness ratio 0.770464430813\n"
     ]
    }
   ],
   "source": [
    "# get global average\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "train_avg_ratio = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "print('avg helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# get average for a user\n",
    "users_outof = dict()\n",
    "users_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_outof[user_id] = users_outof.get(user_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    users_helpful[user_id] = users_helpful.get(user_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "    \n",
    "users_ratio = dict()\n",
    "for user_id in users_outof:\n",
    "    if users_outof[user_id] != 0:\n",
    "        users_ratio[user_id] = users_helpful[user_id] / users_outof[user_id]\n",
    "    else:\n",
    "        users_ratio[user_id] = train_avg_ratio\n",
    "        \n",
    "# get average for a item\n",
    "items_outof = dict()\n",
    "items_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    item_id = d['itemID']\n",
    "    items_outof[item_id] = items_outof.get(item_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    items_helpful[item_id] = items_helpful.get(item_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "\n",
    "items_ratio = dict()\n",
    "for item_id in items_outof:\n",
    "    if items_outof[item_id] != 0:\n",
    "        items_ratio[item_id] = items_helpful[item_id] / items_outof[item_id]\n",
    "    else:\n",
    "        items_ratio[item_id] = train_avg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('betas.pickle') as f:\n",
    "    beta_us, beta_is = pickle.load(f)\n",
    "\n",
    "# get date time statistics\n",
    "def get_y_m_d(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = int(y)\n",
    "    m = int(m)\n",
    "    d = int(d)\n",
    "    return(y, m, d)\n",
    "\n",
    "def get_time_feature(d):\n",
    "    y, m, d = get_y_m_d(d)\n",
    "    y = min(y, 2014)\n",
    "    y = max(y, 1996)\n",
    "    # 1996 [1,0,..,0] 2014 [0,0,...,0]\n",
    "    y_feature = [0] * (2014 - 1996 + 1)\n",
    "    y_feature[y - 1996] = 1\n",
    "    # jan [1,0,...,0] dec [0,0,...,0]\n",
    "    m_feature = [0] * 12\n",
    "    m_feature[m - 1] = 1\n",
    "    # date1 [1,0,...,0] date31 [0,0,...,0]\n",
    "    d_feature = [0] * 31\n",
    "    d_feature[d - 1] = 1\n",
    "    # concatenate\n",
    "    # feature = y_feature[:-1] + m_feature[:-1] + d_feature[:-1]\n",
    "    feature = m_feature[:-1]\n",
    "    return feature\n",
    "\n",
    "def get_feature(d):\n",
    "    feature = [1.0, \n",
    "               users_ratio[d['reviewerID']],\n",
    "               users_ratio[d['reviewerID']] ** 2.,\n",
    "               items_ratio[d['itemID']],\n",
    "               items_ratio[d['itemID']] ** 2.,\n",
    "               len(d['reviewText'].split()),\n",
    "               len(d['reviewText'].split()) ** 2.,\n",
    "               d['rating'],\n",
    "               d['rating'] ** 2.]\n",
    "    return feature\n",
    "\n",
    "# fit a linear regressor to the train ratio\n",
    "def get_feature_and_ratio_label(d, users_ratio, items_ratio):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    if outof == 0:\n",
    "        raise('out of cannot be 0 for ratio')\n",
    "\n",
    "    # get feature and ratio\n",
    "    feature = get_feature(d)\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "    \n",
    "    return (feature, ratio_label)\n",
    "\n",
    "def make_average_regression_dataset(train_data, users_ratio, items_ratio):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label = get_feature_and_ratio_label(d, users_ratio, items_ratio)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return (np.array(features), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build dataset\n",
    "train_xs, train_ys = make_average_regression_dataset(train_data, users_ratio, items_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solve 2-norm minimize problem\n",
    "theta, residuals, rank, s = np.linalg.lstsq(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  1.1415e+05  2.0235e+04  9e+04  1e-17  2e-05  1e+00\n",
      " 1:  1.1415e+05  2.0235e+04  9e+04  2e-16  2e-05  1e+00\n",
      " 2:  1.1415e+05  2.0235e+04  9e+04  2e-16  2e-05  1e+00\n",
      " 3:  1.1415e+05  2.0235e+04  9e+04  2e-16  2e-05  1e+00\n",
      " 4:  1.1415e+05  2.0235e+04  9e+04  2e-16  2e-05  1e+00\n",
      " 5:  1.1415e+05  2.0235e+04  9e+04  2e-16  2e-05  1e+00\n",
      " 6:  1.1415e+05  2.0237e+04  9e+04  3e-16  2e-05  1e+00\n",
      " 7:  1.1416e+05  2.0241e+04  9e+04  3e-16  2e-05  1e+00\n",
      " 8:  1.1416e+05  2.0246e+04  9e+04  3e-16  2e-05  1e+00\n",
      " 9:  1.1419e+05  2.0277e+04  9e+04  3e-16  2e-05  1e+00\n",
      "10:  1.1424e+05  2.0334e+04  9e+04  3e-16  2e-05  1e+00\n",
      "11:  1.1457e+05  2.0685e+04  9e+04  3e-16  2e-05  1e+00\n",
      "12:  1.1550e+05  2.1725e+04  9e+04  3e-16  2e-05  1e+00\n",
      "13:  1.1812e+05  2.5368e+04  9e+04  4e-16  2e-05  9e-01\n",
      "14:  1.2022e+05  2.8776e+04  9e+04  3e-16  2e-05  9e-01\n",
      "15:  1.2346e+05  4.0717e+04  8e+04  4e-16  1e-05  7e-01\n",
      "16:  1.2475e+05  5.3776e+04  7e+04  3e-16  1e-05  6e-01\n",
      "17:  1.2252e+05  6.4503e+04  6e+04  4e-16  1e-05  4e-01\n",
      "18:  1.1938e+05  7.2185e+04  5e+04  3e-16  8e-06  3e-01\n",
      "19:  1.1623e+05  7.8110e+04  4e+04  3e-16  6e-06  2e-01\n",
      "20:  1.1401e+05  8.2186e+04  3e+04  3e-16  5e-06  2e-01\n",
      "21:  1.1100e+05  8.6291e+04  2e+04  3e-16  4e-06  1e-01\n",
      "22:  1.0923e+05  8.8791e+04  2e+04  4e-16  3e-06  1e-01\n",
      "23:  1.0696e+05  9.1475e+04  2e+04  2e-16  3e-06  8e-02\n",
      "24:  1.0537e+05  9.3391e+04  1e+04  3e-16  2e-06  5e-02\n",
      "25:  1.0417e+05  9.4744e+04  9e+03  5e-16  1e-06  4e-02\n",
      "26:  1.0324e+05  9.5725e+04  8e+03  4e-16  1e-06  3e-02\n",
      "27:  1.0249e+05  9.6503e+04  6e+03  5e-16  1e-06  2e-02\n",
      "28:  1.0184e+05  9.7148e+04  5e+03  3e-16  7e-07  2e-02"
     ]
    }
   ],
   "source": [
    "# solve 1-norm minimize problem\n",
    "P = co.matrix(train_xs)\n",
    "q = co.matrix(train_ys.reshape((train_ys.shape[0], 1)))\n",
    "u = l1(P,q)\n",
    "theta = np.array(u).reshape((-1,))\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # using all data\n",
    "# theta = np.array([ -1.53413948e+00,   1.77200914e+00,  -9.14233327e-01,   2.58803493e+00,\n",
    "#                    -1.17773635e+00,  -1.10778809e-06,  -8.47219802e-09,   1.16800069e-01,\n",
    "#                    -1.09893330e-02])\n",
    "\n",
    "# # using train data\n",
    "# theta = np.array([ -1.44115432e+00,   1.74586291e+00,  -8.87805851e-01,   2.41169942e+00,\n",
    "#                    -1.07400193e+00,  -1.48174098e-06,  -9.10511120e-09,   1.08660865e-01,\n",
    "#                    -1.00238718e-02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_occur_stats(valid_data, users_ratio, items_ratio):\n",
    "    stats = dict()\n",
    "    stats['in_user_in_item'] = 0\n",
    "    stats['in_user_not_in_item'] = 0\n",
    "    stats['not_in_user_in_item'] = 0\n",
    "    stats['not_in_user_not_in_item'] = 0\n",
    "    \n",
    "    for d in valid_data:\n",
    "        user_id = d['reviewerID']\n",
    "        item_id = d['itemID']\n",
    "\n",
    "        if (user_id in users_ratio) and (item_id in items_ratio):\n",
    "            stats['in_user_in_item'] += 1\n",
    "        elif (user_id in users_ratio) and (item_id not in items_ratio):\n",
    "            stats['in_user_not_in_item'] += 1\n",
    "        elif (user_id not in users_ratio) and (item_id in items_ratio):\n",
    "            stats['not_in_user_in_item'] += 1\n",
    "        else:\n",
    "            stats['not_in_user_not_in_item']\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    \n",
    "    if (user_id in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = np.dot(get_feature(d), theta)\n",
    "    elif (user_id in users_ratio) and (item_id not in items_ratio):\n",
    "        ratio = users_ratio[user_id]\n",
    "    elif (user_id not in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = items_ratio[item_id]\n",
    "    else:\n",
    "        ratio = train_avg_ratio\n",
    "    return ratio * outof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_valid_mae(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(get_occur_stats(valid_data, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############ produce test ############\n",
    "\n",
    "# # load helpful_data.json\n",
    "# test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "\n",
    "# # on test set\n",
    "# test_helpfuls_predict = [predict_helpful(d, theta, train_avg_ratio, users_ratio, items_ratio) for d in test_data]\n",
    "\n",
    "# # load 'pairs_Helpful.txt'\n",
    "# # get header_str and user_item_outofs\n",
    "# with open('pairs_Helpful.txt') as f:\n",
    "#     # read and strip lines\n",
    "#     lines = [l.strip() for l in f.readlines()]\n",
    "#     # stirip out the headers\n",
    "#     header_str = lines.pop(0)\n",
    "#     # get a list of user_item_ids\n",
    "#     user_item_outofs = [l.split('-') for l in lines]\n",
    "#     user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "    \n",
    "# # make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "# for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "#     assert d['reviewerID'] == user_id\n",
    "#     assert d['itemID'] == item_id\n",
    "#     assert d['helpful']['outOf'] == outof\n",
    "    \n",
    "# # write to output file\n",
    "# f = open('predictions_Helpful.txt', 'w')\n",
    "# print(header_str, file=f)\n",
    "# for (user_id, item_id, outof), test_helpful_predict in zip(user_item_outofs, test_helpfuls_predict):\n",
    "#     print('%s-%s-%s,%s' % (user_id, item_id, int(outof), test_helpful_predict), file=f)\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
