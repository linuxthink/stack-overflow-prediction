{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "# data\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'\n",
    "\n",
    "# l1-norm\n",
    "import cvxopt as co\n",
    "from l1 import l1\n",
    "\n",
    "# natural language processing\n",
    "import nltk\n",
    "import nltk.data\n",
    "import string\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# xgboosting\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # load all_data\n",
    "# start_time = time.time()\n",
    "# all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "# print(time.time() - start_time)\n",
    "\n",
    "# # split training and valid set\n",
    "# # all\n",
    "# all_size = len(all_data)\n",
    "\n",
    "# # train\n",
    "# train_size = 900000\n",
    "# # train_size = all_size # uncomment this to produce test\n",
    "# train_data = all_data[:train_size]\n",
    "\n",
    "# # valid\n",
    "# valid_size = 100000\n",
    "# valid_data = all_data[all_size - valid_size:]\n",
    "\n",
    "# # remove the outlier\n",
    "# for i in reversed(range(train_size)):\n",
    "#     d = train_data[i] \n",
    "#     if d['helpful']['outOf'] > 5000:\n",
    "#         train_data.pop(i)\n",
    "        \n",
    "# # utility functions\n",
    "# def get_mae(helpfuls, helpfuls_predict):\n",
    "#     return np.sum(np.fabs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]\n",
    "\n",
    "# # get global average\n",
    "# train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "# train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "# train_avg_ratio = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "# print('avg helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# # linear search best ratio\n",
    "# def linear_search_ratio(helpfuls, outofs, search_range=(0.3, 1.0, 0.001)):\n",
    "#     alphas = np.arange(*search_range)\n",
    "#     errors = [get_mae(helpfuls, outofs * alpha) for alpha in alphas]\n",
    "#     optimal_alpha = alphas[np.argmin(errors)]\n",
    "#     return optimal_alpha\n",
    "\n",
    "# # training set global\n",
    "# train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "# train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "# train_avg_ratio = linear_search_ratio(train_helpfuls, train_outofs, search_range=(0.3, 1.0, 0.001))\n",
    "# print('optimal helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# # get average for a user\n",
    "# users_outof = dict()\n",
    "# users_helpful = dict()\n",
    "\n",
    "# for d in train_data:\n",
    "#     user_id = d['reviewerID']\n",
    "#     users_outof[user_id] = users_outof.get(user_id, 0.0) + float(d['helpful']['outOf'])\n",
    "#     users_helpful[user_id] = users_helpful.get(user_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "    \n",
    "# users_ratio = dict()\n",
    "# for user_id in users_outof:\n",
    "#     if users_outof[user_id] != 0:\n",
    "#         users_ratio[user_id] = users_helpful[user_id] / users_outof[user_id]\n",
    "#     else:\n",
    "#         users_ratio[user_id] = train_avg_ratio\n",
    "        \n",
    "# # get average for a item\n",
    "# items_outof = dict()\n",
    "# items_helpful = dict()\n",
    "\n",
    "# for d in train_data:\n",
    "#     item_id = d['itemID']\n",
    "#     items_outof[item_id] = items_outof.get(item_id, 0.0) + float(d['helpful']['outOf'])\n",
    "#     items_helpful[item_id] = items_helpful.get(item_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "\n",
    "# items_ratio = dict()\n",
    "# for item_id in items_outof:\n",
    "#     if items_outof[item_id] != 0:\n",
    "#         items_ratio[item_id] = items_helpful[item_id] / items_outof[item_id]\n",
    "#     else:\n",
    "#         items_ratio[item_id] = train_avg_ratio\n",
    "        \n",
    "# # pre-computed features\n",
    "# with open('betas.pickle') as f:\n",
    "#     beta_us, beta_is = pickle.load(f)\n",
    "    \n",
    "# with open('train_ratio_list.pickle') as f:\n",
    "#     train_ratio_list = pickle.load(f)\n",
    "    \n",
    "# with open(os.path.join(data_root, 'num_unique_word.feature')) as f:\n",
    "#     num_unique_word_dict = pickle.load(f)\n",
    "    \n",
    "# with open(os.path.join(data_root, 'style_dict.feature')) as f:\n",
    "#     style_dict = pickle.load(f)\n",
    "#     # style_dict['U243261361']['I572782694']\n",
    "#     # {'avg_word_len': 4.857142857142857,\n",
    "#     #  'capital_count': 11.0,\n",
    "#     #  'capital_ratio': 0.028205128205128206,\n",
    "#     #  'dotdotdot_count': 4.0,\n",
    "#     #  'exclam_count': 0.0,\n",
    "#     #  'exclam_exclam_count': 0.0,\n",
    "#     #  'num_chars': 369.0,\n",
    "#     #  'num_sentences': 3.0,\n",
    "#     #  'num_unique_words': 50,\n",
    "#     #  'num_words': 63.0,\n",
    "#     #  'num_words_summary': 2,\n",
    "#     #  'punctuation_count': 21.0,\n",
    "#     #  'punctuation_ratio': 0.05384615384615385,\n",
    "#     #  'question_count': 0.0,\n",
    "#     #  'redability': 16.65714285714285}\n",
    "    \n",
    "# # feature engineering\n",
    "# # get date time statistics\n",
    "# def get_y_m_d(d):\n",
    "#     unix_time = d['unixReviewTime']\n",
    "#     y, m, d = datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d').split('-')\n",
    "#     y = int(y)\n",
    "#     m = int(m)\n",
    "#     d = int(d)\n",
    "#     return(y, m, d)\n",
    "\n",
    "# def get_feature_time(d):\n",
    "#     y, m, d = get_y_m_d(d)\n",
    "#     y = min(y, 2014)\n",
    "#     y = max(y, 1996)\n",
    "#     # 1996 [1,0,..,0] 2014 [0,0,...,0]\n",
    "#     y_feature = [0] * (2014 - 1996 + 1)\n",
    "#     y_feature[y - 1996] = 1\n",
    "#     # jan [1,0,...,0] dec [0,0,...,0]\n",
    "#     m_feature = [0] * 12\n",
    "#     m_feature[m - 1] = 1\n",
    "#     # date1 [1,0,...,0] date31 [0,0,...,0]\n",
    "#     d_feature = [0] * 31\n",
    "#     d_feature[d - 1] = 1\n",
    "#     # concatenate\n",
    "#     feature = y_feature[:-1] + m_feature[:-1] + d_feature[:-1]\n",
    "#     return feature\n",
    "\n",
    "# def get_num_uique_word(d):\n",
    "#     wordCount = defaultdict(int)\n",
    "#     for w in d[\"reviewText\"].split():\n",
    "#         w = \"\".join([c for c in w.lower() if not c in punctuation])\n",
    "#         w = stemmer.stem(w)\n",
    "#         wordCount[w] += 1\n",
    "#     return len(wordCount)\n",
    "\n",
    "# def get_feature(d):\n",
    "#     user_id = d['reviewerID']\n",
    "#     item_id = d['itemID']\n",
    "    \n",
    "#     feature = [1.0]\n",
    "#     feature += [users_ratio[user_id], items_ratio[item_id]]\n",
    "#     feature += [float(d['rating'])]\n",
    "    \n",
    "#     s = style_dict[user_id][item_id]\n",
    "# #     feature += [s['redability'], s['num_unique_words'], s['num_words_summary'], s['num_words'], s['capital_count']]\n",
    "#     feature += [s['num_words'], s['redability'], s['exclam_exclam_count']+s['question_count']]\n",
    "    \n",
    "#     return feature\n",
    "\n",
    "\n",
    "# # get [feature, label] from single datum\n",
    "# def get_feature_and_ratio_label(d, users_ratio, items_ratio):\n",
    "#     # check valid\n",
    "#     outof = float(d['helpful']['outOf'])\n",
    "#     if outof == 0:\n",
    "#         raise('out of cannot be 0 for ratio')\n",
    "\n",
    "#     # get feature and ratio\n",
    "#     feature = get_feature(d)\n",
    "#     ratio_label = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "#     return (feature, ratio_label)\n",
    "\n",
    "# # build [feature, label] list from entire dataset\n",
    "# def make_average_regression_dataset(train_data, users_ratio, items_ratio):\n",
    "#     features = []\n",
    "#     labels = []\n",
    "    \n",
    "#     for d in train_data:\n",
    "#         if float(d['helpful']['outOf']) == 0:\n",
    "#             continue\n",
    "#         feature, label = get_feature_and_ratio_label(d, users_ratio, items_ratio)\n",
    "#         features.append(feature)\n",
    "#         labels.append(label)\n",
    "#     return (np.array(features), np.array(labels))\n",
    "\n",
    "# # make one prediction\n",
    "# def predict_helpful(d, ratio_predictor, train_avg_ratio, users_ratio, items_ratio):\n",
    "#     # ratio_predictor[func]: y = ratio_predictor(get_feature(d))\n",
    "#     user_id = d['reviewerID']\n",
    "#     item_id = d['itemID']\n",
    "#     outof = float(d['helpful']['outOf'])\n",
    "    \n",
    "#     if (user_id in users_ratio) and (item_id in items_ratio):\n",
    "#         # ratio = np.dot(get_feature(d), theta)\n",
    "#         predict = ratio_predictor(get_feature(d))\n",
    "#         ratio = predict[0] # np.ndarray\n",
    "#     elif (user_id in users_ratio) and (item_id not in items_ratio):\n",
    "#         ratio = users_ratio[user_id]\n",
    "#     elif (user_id not in users_ratio) and (item_id in items_ratio):\n",
    "#         ratio = items_ratio[item_id]\n",
    "#     else:\n",
    "#         ratio = train_avg_ratio\n",
    "#     return ratio * outof\n",
    "\n",
    "# # make predictions and get mae on a dataset\n",
    "# def get_valid_mae(valid_data, ratio_predictor, train_avg_ratio, users_ratio, items_ratio):\n",
    "#     # ground truth nhelpful\n",
    "#     helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "#     # predited nhelpful\n",
    "#     helpfuls_predict = np.array([predict_helpful(d, ratio_predictor, train_avg_ratio, users_ratio, items_ratio) for d in valid_data])\n",
    "#     # return mae\n",
    "#     return get_mae(helpfuls, helpfuls_predict)\n",
    "\n",
    "# # build dataset\n",
    "# train_xs, train_ys = make_average_regression_dataset(train_data, users_ratio, items_ratio)\n",
    "# valid_xs, valid_ys = make_average_regression_dataset(valid_data, users_ratio, items_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump((train_xs, train_ys, valid_xs, valid_ys), open(\"train_xs_train_ys_valid_xs_valid_ys.pickle\", \"wb\"))\n",
    "train_xs, train_ys, valid_xs, valid_ys = pickle.load( open(\"train_xs_train_ys_valid_xs_valid_ys.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "# cut = 5000\n",
    "dtrain = xgb.DMatrix(train_xs, label=train_ys) # can set weight\n",
    "dvalid = xgb.DMatrix(valid_xs, label=valid_ys) # can set weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'silent':1,\n",
    "         'objective':'reg:logistic',\n",
    "         'nthread': 7,\n",
    "         'bst:eta':0.1\n",
    "         'bst:max_depth':6,\n",
    "         }\n",
    "\n",
    "plst = param.items()\n",
    "plst += [('eval_metric', 'auc')] # Multiple evals can be handled in this way\n",
    "plst += [('eval_metric', 'ams@0')]\n",
    "\n",
    "watchlist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "bst = xgb.train(param, dtrain, 100, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_xs_train_ys_valid_xs_valid_ys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
