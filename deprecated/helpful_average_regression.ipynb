{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "# data\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'\n",
    "\n",
    "# l1-norm\n",
    "import cvxopt as co\n",
    "from l1 import l1\n",
    "\n",
    "# # natural language processing\n",
    "# import nltk\n",
    "# import nltk.data\n",
    "# import string\n",
    "# sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# stemmer = nltk.stem.porter.PorterStemmer()\n",
    "# punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.9671149254\n"
     ]
    }
   ],
   "source": [
    "# load all_data\n",
    "start_time = time.time()\n",
    "all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split training and valid set\n",
    "# all\n",
    "all_size = len(all_data)\n",
    "\n",
    "# train\n",
    "train_size = 900000\n",
    "# train_size = all_size # uncomment this to produce test\n",
    "train_data = all_data[:train_size]\n",
    "\n",
    "# valid\n",
    "valid_size = 100000\n",
    "valid_data = all_data[all_size - valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "for i in reversed(range(train_size)):\n",
    "    d = train_data[i] \n",
    "    if d['helpful']['outOf'] > 5000:\n",
    "        train_data.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def get_mae(helpfuls, helpfuls_predict):\n",
    "    return np.sum(np.fabs(helpfuls_predict - helpfuls.astype(float))) / helpfuls.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg helpfulness ratio 0.768819898316\n",
      "optimal helpfulness ratio 0.856\n"
     ]
    }
   ],
   "source": [
    "# get global average\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "train_avg_ratio = np.sum(train_helpfuls) / np.sum(train_outofs.astype(float))\n",
    "print('avg helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# linear search best ratio\n",
    "def linear_search_ratio(helpfuls, outofs, search_range=(0.3, 1.0, 0.001)):\n",
    "    alphas = np.arange(*search_range)\n",
    "    errors = [get_mae(helpfuls, outofs * alpha) for alpha in alphas]\n",
    "    optimal_alpha = alphas[np.argmin(errors)]\n",
    "    return optimal_alpha\n",
    "\n",
    "# training set global\n",
    "train_helpfuls = np.array([d['helpful']['nHelpful'] for d in train_data])\n",
    "train_outofs =  np.array([d['helpful']['outOf'] for d in train_data])\n",
    "train_avg_ratio = linear_search_ratio(train_helpfuls, train_outofs, search_range=(0.3, 1.0, 0.001))\n",
    "print('optimal helpfulness ratio', train_avg_ratio)\n",
    "\n",
    "# get average for a user\n",
    "users_outof = dict()\n",
    "users_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    user_id = d['reviewerID']\n",
    "    users_outof[user_id] = users_outof.get(user_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    users_helpful[user_id] = users_helpful.get(user_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "    \n",
    "users_ratio = dict()\n",
    "for user_id in users_outof:\n",
    "    if users_outof[user_id] != 0:\n",
    "        users_ratio[user_id] = users_helpful[user_id] / users_outof[user_id]\n",
    "    else:\n",
    "        users_ratio[user_id] = train_avg_ratio\n",
    "        \n",
    "# get average for a item\n",
    "items_outof = dict()\n",
    "items_helpful = dict()\n",
    "\n",
    "for d in train_data:\n",
    "    item_id = d['itemID']\n",
    "    items_outof[item_id] = items_outof.get(item_id, 0.0) + float(d['helpful']['outOf'])\n",
    "    items_helpful[item_id] = items_helpful.get(item_id, 0.0) + float(d['helpful']['nHelpful'])\n",
    "\n",
    "items_ratio = dict()\n",
    "for item_id in items_outof:\n",
    "    if items_outof[item_id] != 0:\n",
    "        items_ratio[item_id] = items_helpful[item_id] / items_outof[item_id]\n",
    "    else:\n",
    "        items_ratio[item_id] = train_avg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre-computed features\n",
    "with open('betas.pickle') as f:\n",
    "    beta_us, beta_is = pickle.load(f)\n",
    "    \n",
    "with open('train_ratio_list.pickle') as f:\n",
    "    train_ratio_list = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root, 'num_unique_word.feature')) as f:\n",
    "    num_unique_word_dict = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(data_root, 'style_dict.feature')) as f:\n",
    "    style_dict = pickle.load(f)\n",
    "    # style_dict['U243261361']['I572782694']\n",
    "    # {'avg_word_len': 4.857142857142857,\n",
    "    #  'capital_count': 11.0,\n",
    "    #  'capital_ratio': 0.028205128205128206,\n",
    "    #  'dotdotdot_count': 4.0,\n",
    "    #  'exclam_count': 0.0,\n",
    "    #  'exclam_exclam_count': 0.0,\n",
    "    #  'num_chars': 369.0,\n",
    "    #  'num_sentences': 3.0,\n",
    "    #  'num_unique_words': 50,\n",
    "    #  'num_words': 63.0,\n",
    "    #  'num_words_summary': 2,\n",
    "    #  'punctuation_count': 21.0,\n",
    "    #  'punctuation_ratio': 0.05384615384615385,\n",
    "    #  'question_count': 0.0,\n",
    "    #  'redability': 16.65714285714285}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "# get date time statistics\n",
    "def get_y_m_d(d):\n",
    "    unix_time = d['unixReviewTime']\n",
    "    y, m, d = datetime.datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d').split('-')\n",
    "    y = float(y)\n",
    "    m = float(m)\n",
    "    d = float(d)\n",
    "    return(y, m, d)\n",
    "\n",
    "def get_feature_time(d):\n",
    "    y, m, d = get_y_m_d(d)\n",
    "    # y = min(y, 2014)\n",
    "    # y = max(y, 1996)\n",
    "    # # 1996 [1,0,..,0] 2014 [0,0,...,0]\n",
    "    # y_feature = [0] * (2014 - 1996 + 1)\n",
    "    # y_feature[y - 1996] = 1\n",
    "    # # jan [1,0,...,0] dec [0,0,...,0]\n",
    "    # m_feature = [0] * 12\n",
    "    # m_feature[m - 1] = 1\n",
    "    # # date1 [1,0,...,0] date31 [0,0,...,0]\n",
    "    # d_feature = [0] * 31\n",
    "    # d_feature[d - 1] = 1\n",
    "    # # concatenate\n",
    "    # feature = y_feature[:-1] + m_feature[:-1] + d_feature[:-1]\n",
    "    return [y, m, d]\n",
    "\n",
    "def get_num_uique_word(d):\n",
    "    wordCount = defaultdict(int)\n",
    "    for w in d[\"reviewText\"].split():\n",
    "        w = \"\".join([c for c in w.lower() if not c in punctuation])\n",
    "        w = stemmer.stem(w)\n",
    "        wordCount[w] += 1\n",
    "    return len(wordCount)\n",
    "\n",
    "def get_feature(d):\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    \n",
    "    # offset\n",
    "    feature = [1.0]\n",
    "    # user, item average\n",
    "    feature += [users_ratio[user_id], items_ratio[item_id]]\n",
    "    # rating\n",
    "    feature += [float(d['rating'])]\n",
    "    # styles\n",
    "    s = style_dict[user_id][item_id]\n",
    "    feature += [s['num_words'], s['redability'], s['exclam_exclam_count']+s['question_count']]\n",
    "    # time\n",
    "    feature += get_feature_time(d)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get [feature, label] from single datum\n",
    "def get_feature_and_ratio_label(d, users_ratio, items_ratio):\n",
    "    # check valid\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    if outof == 0:\n",
    "        raise('out of cannot be 0 for ratio')\n",
    "\n",
    "    # get feature and ratio\n",
    "    feature = get_feature(d)\n",
    "    ratio_label = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "    return (feature, ratio_label)\n",
    "\n",
    "# build [feature, label] list from entire dataset\n",
    "def make_average_regression_dataset(train_data, users_ratio, items_ratio):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for d in train_data:\n",
    "        if float(d['helpful']['outOf']) == 0:\n",
    "            continue\n",
    "        feature, label = get_feature_and_ratio_label(d, users_ratio, items_ratio)\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    return (np.array(features), np.array(labels))\n",
    "\n",
    "# make one prediction\n",
    "def predict_helpful(d, ratio_predictor, train_avg_ratio, users_ratio, items_ratio):\n",
    "    # ratio_predictor[func]: y = ratio_predictor(get_feature(d))\n",
    "    user_id = d['reviewerID']\n",
    "    item_id = d['itemID']\n",
    "    outof = float(d['helpful']['outOf'])\n",
    "    \n",
    "    if (user_id in users_ratio) and (item_id in items_ratio):\n",
    "        # ratio = np.dot(get_feature(d), theta)\n",
    "        predict = ratio_predictor(get_feature(d))\n",
    "        ratio = predict[0] # np.ndarray\n",
    "    elif (user_id in users_ratio) and (item_id not in items_ratio):\n",
    "        ratio = users_ratio[user_id]\n",
    "    elif (user_id not in users_ratio) and (item_id in items_ratio):\n",
    "        ratio = items_ratio[item_id]\n",
    "    else:\n",
    "        ratio = train_avg_ratio\n",
    "    return ratio * outof\n",
    "\n",
    "# make predictions and get mae on a dataset\n",
    "def get_valid_mae(valid_data, ratio_predictor, train_avg_ratio, users_ratio, items_ratio):\n",
    "    # ground truth nhelpful\n",
    "    helpfuls = np.array([float(d['helpful']['nHelpful']) for d in valid_data])\n",
    "    # predited nhelpful\n",
    "    helpfuls_predict = np.array([predict_helpful(d, ratio_predictor, train_avg_ratio, users_ratio, items_ratio) for d in valid_data])\n",
    "    # return mae\n",
    "    return get_mae(helpfuls, helpfuls_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build dataset\n",
    "# train_xs, train_ys = make_average_regression_dataset(train_data, users_ratio, items_ratio)\n",
    "# valid_xs, valid_ys = make_average_regression_dataset(valid_data, users_ratio, items_ratio)\n",
    "all_xs, all_ys = make_average_regression_dataset(all_data, users_ratio, items_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 2-norm linear regression problem\n",
    "# class RegressorTwoNorm():\n",
    "#     def __init__(self):\n",
    "#         self.theta = None\n",
    "#         self.resitudals = None\n",
    "#         self.residuals = None\n",
    "#         self.s = None\n",
    "        \n",
    "#     def fit(self, xs, ys):\n",
    "#         self.theta, self.residuals, self.rank, self.s = np.linalg.lstsq(xs, ys)\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         return np.array(np.dot(x, self.theta)).reshape((-1,))\n",
    "\n",
    "# regressor_two_norm = RegressorTwoNorm()\n",
    "# regressor_two_norm.fit(train_xs, train_ys)\n",
    "# print(get_valid_mae(valid_data, regressor_two_norm, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# regressor_linear = LinearRegression()\n",
    "# regressor_linear.fit(train_xs, train_ys)\n",
    "# print(get_valid_mae(valid_data, regressor_linear.predict, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 1-norm predictor\n",
    "# class RegressorOneNorm():\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def fit(self, xs, ys):\n",
    "#         self.P = co.matrix(xs)\n",
    "#         self.q = co.matrix(ys.reshape((ys.shape[0], 1)))\n",
    "#         self.u = l1(self.P, self.q)\n",
    "#         self.theta = np.array(self.u).reshape((-1,))\n",
    "    \n",
    "#     def __call__(self, x):\n",
    "#         return np.array(np.dot(x, self.theta)).reshape((-1,))\n",
    "\n",
    "# regressor_one_norm = RegressorOneNorm()\n",
    "# regressor_one_norm.fit(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(get_valid_mae(valid_data, regressor_one_norm, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# regressor_rf = RandomForestRegressor()\n",
    "# regressor_rf.fit(train_xs, train_ys)\n",
    "# print(get_valid_mae(valid_data, regressor_rf.predict, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# regressor_gb = GradientBoostingRegressor(learning_rate=0.001, n_estimators=1000, max_depth=6, loss='lad')\n",
    "# regressor_gb.fit(train_xs[:5000], train_ys[:5000])\n",
    "\n",
    "# print(get_valid_mae(valid_data, regressor_gb.predict, train_avg_ratio, users_ratio, items_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############ produce test ############\n",
    "\n",
    "# # load helpful_data.json\n",
    "# test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "\n",
    "# # on test set\n",
    "# test_helpfuls_predict = [predict_helpful(d, regressor_gb.predict, train_avg_ratio, users_ratio, items_ratio) for d in test_data]\n",
    "\n",
    "# # load 'pairs_Helpful.txt'\n",
    "# # get header_str and user_item_outofs\n",
    "# with open('pairs_Helpful.txt') as f:\n",
    "#     # read and strip lines\n",
    "#     lines = [l.strip() for l in f.readlines()]\n",
    "#     # stirip out the headers\n",
    "#     header_str = lines.pop(0)\n",
    "#     # get a list of user_item_ids\n",
    "#     user_item_outofs = [l.split('-') for l in lines]\n",
    "#     user_item_outofs = [[d[0], d[1], float(d[2])] for d in user_item_outofs]\n",
    "    \n",
    "# # make sure `data.json` and `pairs_Helpful.txt` the same order\n",
    "# for (user_id, item_id, outof), d in zip(user_item_outofs, test_data):\n",
    "#     assert d['reviewerID'] == user_id\n",
    "#     assert d['itemID'] == item_id\n",
    "#     assert d['helpful']['outOf'] == outof\n",
    "    \n",
    "# # write to output file\n",
    "# f = open('predictions_Helpful.txt', 'w')\n",
    "# print(header_str, file=f)\n",
    "# for (user_id, item_id, outof), test_helpful_predict in zip(user_item_outofs, test_helpfuls_predict):\n",
    "#     print('%s-%s-%s,%s' % (user_id, item_id, int(outof), test_helpful_predict), file=f)\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
