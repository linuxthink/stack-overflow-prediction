{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "from neon.data import DataIterator, Text, load_text\n",
    "from neon.initializers import Uniform, GlorotUniform\n",
    "from neon.layers import GeneralizedCost, LSTM, Affine, Dropout, LookupTable, RecurrentSum\n",
    "from neon.models import Model\n",
    "from neon.optimizers import Adagrad\n",
    "from neon.transforms import Logistic, Tanh, Softmax, CrossEntropyMulti, Accuracy\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "from neon.util.argparser import NeonArgparser\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import os\n",
    "data_root = os.path.expanduser(\"~\") + '/data/CSE255/'\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "stemmer = PorterStemmer()\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.1929328442\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_data = pickle.load(open(data_root + \"all_data.pickle\", \"rb\"))\n",
    "test_data = pickle.load(open(data_root + \"helpful_data.pickle\", \"rb\"))\n",
    "all_and_test_data = all_data + test_data\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_size = len(all_data)\n",
    "train_size = 900000\n",
    "# train_size = all_size\n",
    "valid_size = 100000\n",
    "train_data = all_data[:train_size]\n",
    "valid_data = all_data[all_size - valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dictionary\n",
    "imdb_word_to_idx = pickle.load(open(data_root + \"imdb.dict.pkl\", \"rb\"))\n",
    "imdb_idx_to_word = dict()\n",
    "for word, idx in imdb_word_to_idx.iteritems():\n",
    "    imdb_idx_to_word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1000000\n",
      "10000 of 1000000\n",
      "20000 of 1000000\n",
      "30000 of 1000000\n",
      "40000 of 1000000\n",
      "50000 of 1000000\n",
      "60000 of 1000000\n",
      "70000 of 1000000\n",
      "80000 of 1000000\n",
      "90000 of 1000000\n",
      "100000 of 1000000\n",
      "110000 of 1000000\n",
      "120000 of 1000000\n",
      "130000 of 1000000\n",
      "140000 of 1000000\n",
      "150000 of 1000000\n",
      "160000 of 1000000\n",
      "170000 of 1000000\n",
      "180000 of 1000000\n",
      "190000 of 1000000\n",
      "200000 of 1000000\n",
      "210000 of 1000000\n",
      "220000 of 1000000\n",
      "230000 of 1000000\n",
      "240000 of 1000000\n",
      "250000 of 1000000\n",
      "260000 of 1000000\n",
      "270000 of 1000000\n",
      "280000 of 1000000\n",
      "290000 of 1000000\n",
      "300000 of 1000000\n",
      "310000 of 1000000\n",
      "320000 of 1000000\n",
      "330000 of 1000000\n",
      "340000 of 1000000\n",
      "350000 of 1000000\n",
      "360000 of 1000000\n",
      "370000 of 1000000\n",
      "380000 of 1000000\n",
      "390000 of 1000000\n",
      "400000 of 1000000\n",
      "410000 of 1000000\n",
      "420000 of 1000000\n",
      "430000 of 1000000\n",
      "440000 of 1000000\n",
      "450000 of 1000000\n",
      "460000 of 1000000\n",
      "470000 of 1000000\n",
      "480000 of 1000000\n",
      "490000 of 1000000\n",
      "500000 of 1000000\n",
      "510000 of 1000000\n",
      "520000 of 1000000\n",
      "530000 of 1000000\n",
      "540000 of 1000000\n",
      "550000 of 1000000\n",
      "560000 of 1000000\n",
      "570000 of 1000000\n",
      "580000 of 1000000\n",
      "590000 of 1000000\n",
      "600000 of 1000000\n",
      "610000 of 1000000\n",
      "620000 of 1000000\n",
      "630000 of 1000000\n",
      "640000 of 1000000\n",
      "650000 of 1000000\n",
      "660000 of 1000000\n",
      "670000 of 1000000\n",
      "680000 of 1000000\n",
      "690000 of 1000000\n",
      "700000 of 1000000\n",
      "710000 of 1000000\n",
      "720000 of 1000000\n",
      "730000 of 1000000\n",
      "740000 of 1000000\n",
      "750000 of 1000000\n",
      "760000 of 1000000\n",
      "770000 of 1000000\n",
      "780000 of 1000000\n",
      "790000 of 1000000\n",
      "800000 of 1000000\n",
      "810000 of 1000000\n",
      "820000 of 1000000\n",
      "830000 of 1000000\n",
      "840000 of 1000000\n",
      "850000 of 1000000\n",
      "860000 of 1000000\n",
      "870000 of 1000000\n",
      "880000 of 1000000\n",
      "890000 of 1000000\n",
      "900000 of 1000000\n",
      "910000 of 1000000\n",
      "920000 of 1000000\n",
      "930000 of 1000000\n",
      "940000 of 1000000\n",
      "950000 of 1000000\n",
      "960000 of 1000000\n",
      "970000 of 1000000\n",
      "980000 of 1000000\n",
      "990000 of 1000000\n",
      "done adding samples\n"
     ]
    }
   ],
   "source": [
    "# convert data to word indecies\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# set the dataset!\n",
    "taget_data = all_data\n",
    "\n",
    "def data_to_word_index(d, include_summary=True, random_crop=False):\n",
    "    # split\n",
    "    if include_summary:\n",
    "        review_text = d['summary'].split() + d['reviewText'].split()\n",
    "    else:\n",
    "        review_text = d['reviewText'].split()\n",
    "        \n",
    "    # pre-process text\n",
    "    review_text = [w.lower() for w in review_text if w not in punctuation]\n",
    "    review_text = [\"\".join(c for c in w if c not in punctuation) for w in review_text]\n",
    "    \n",
    "    # review_text = [stemmer.stem(w) for w in review_text]\n",
    "    review_text = [w for w in review_text if w in imdb_word_to_idx]\n",
    "    \n",
    "    if random_crop:\n",
    "        # random_crop\n",
    "        if len(review_text) >= 150:\n",
    "            start_index = random.randint(0,int(len(review_text) / 2))\n",
    "        else:\n",
    "            start_index = random.randint(0,int(len(review_text) / 3))\n",
    "        review_text = review_text[start_index:]\n",
    "    \n",
    "    # convert to index\n",
    "    review_text_idx = [imdb_word_to_idx[w] for w in review_text]\n",
    "    review_text_idx = np.array(review_text_idx).astype(int)\n",
    "    \n",
    "    return review_text_idx\n",
    "\n",
    "def data_to_label(d):\n",
    "    ratio = float(d['helpful']['nHelpful']) / float(d['helpful']['outOf'])\n",
    "    if ratio >= 0.499:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for idx, d in enumerate(taget_data):\n",
    "    if idx % 10000 == 0:\n",
    "        print(\"%d of %d\" % (idx, len(taget_data)))\n",
    "    \n",
    "    if float(d['helpful']['outOf']) == 0:\n",
    "        continue\n",
    "    \n",
    "    features.append(data_to_word_index(d, include_summary=True))\n",
    "    labels.append(data_to_label(d))\n",
    "    \n",
    "print(\"done adding samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ramaining: 350000\n",
      "ramaining: 340000\n",
      "ramaining: 330000\n",
      "ramaining: 320000\n",
      "ramaining: 310000\n",
      "ramaining: 300000\n",
      "ramaining: 290000\n",
      "ramaining: 280000\n",
      "ramaining: 270000\n",
      "ramaining: 260000\n",
      "ramaining: 250000\n",
      "ramaining: 240000\n",
      "ramaining: 230000\n",
      "ramaining: 220000\n",
      "ramaining: 210000\n",
      "ramaining: 200000\n",
      "ramaining: 190000\n",
      "ramaining: 180000\n",
      "ramaining: 170000\n",
      "ramaining: 160000\n",
      "ramaining: 150000\n",
      "ramaining: 140000\n",
      "ramaining: 130000\n",
      "ramaining: 120000\n",
      "ramaining: 110000\n",
      "ramaining: 100000\n",
      "ramaining: 90000\n",
      "ramaining: 80000\n",
      "ramaining: 70000\n",
      "ramaining: 60000\n",
      "ramaining: 50000\n",
      "ramaining: 40000\n",
      "ramaining: 30000\n",
      "ramaining: 20000\n",
      "ramaining: 10000\n",
      "ramaining: 0\n",
      "done fixing bias\n"
     ]
    }
   ],
   "source": [
    "# compansate for the bias of negative\n",
    "random.seed(0)\n",
    "positive_count = sum(1 for l in labels if l == 1)\n",
    "negative_count = sum(1 for l in labels if l == 0)\n",
    "assert positive_count + negative_count == len(labels)\n",
    "\n",
    "bias_count = positive_count - negative_count\n",
    "\n",
    "while bias_count > 0:\n",
    "    d = random.choice(taget_data)\n",
    "    \n",
    "    if (float(d['helpful']['outOf']) == 0 or data_to_label(d) == 1):\n",
    "        continue\n",
    "        \n",
    "    features.append(data_to_word_index(d, random_crop=True))\n",
    "    labels.append(data_to_label(d))\n",
    "    \n",
    "    bias_count -= 1\n",
    "    if bias_count % 10000 == 0:\n",
    "        print(\"ramaining: %d\" % (bias_count))\n",
    "\n",
    "positive_count = sum(1 for l in labels if l == 1)\n",
    "negative_count = sum(1 for l in labels if l == 0)\n",
    "assert positive_count == negative_count\n",
    "\n",
    "print(\"done fixing bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump\n",
    "pickle.dump((features, labels), open(data_root + \"train_valid_text_index_in_binary_label.pickle\", \"wb\"), \n",
    "            protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle and then dump\n",
    "random.seed(0)\n",
    "\n",
    "combined = zip(features, labels)\n",
    "random.shuffle(combined)\n",
    "\n",
    "features[:], labels[:] = zip(*combined)\n",
    "\n",
    "pickle.dump((features, labels), open(data_root + \"train_valid_text_index_in_binary_label_shuffled.pickle\", \"wb\"), \n",
    "            protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (features, labels) = pickle.load(open(data_root + \"train_text_data.pickle\", \"rb\"))\n",
    "# pickle.dump((features[:100000], ratios[:100000]), \n",
    "#             open(data_root + \"train_text_data_100000.pickle\", \"wb\"), \n",
    "#             protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
